{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes and edges\n",
    "Going back to familiar binary Logistic Regression, we visualize, let's say, a model trained on the dataset having 3 features and a single label. On the graph, each feature/label ($\\mathbf{x}$ or $\\mathbf{y}$) is represented by a *node* and each model weight ($w_1,w_2,w_3$) is represented by a *colored edge*. The bias $w_0$ (or sometimes denoted $b$) is not showing on the graph, but keep in mind it is attached to the output node. This is the most basic architecture of a Neural Network with 4 parameters (3 weights + 1 bias).\n",
    "\n",
    "<img src='image/mlp_linear_simple.png' style='height:180px; margin:20px auto;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "Now, we extend the problem to a Stacking model, where 5 base models and the meta model are all Logistic Regression. Beside an input layer and an output layer, there is a new layer between them, called the *hidden layer*. We can add more and more hidden layers for multilevel stacking design. By doing this, our Neural Network becomes *deeper* and can capture more complicated relationship in our data. This opens up a new branch of Machine Learning algorithms: Deep Learning.\n",
    "\n",
    "<img src='image/mlp_linear_stacking.png' style='height:300px; margin:20px auto;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation learning\n",
    "In the two examples above, the Neural Network is designed for a binary classification problem, where the target is a vector storing the probabilities of being classified to the positive class. For a multi-class classification problem, we need to contruct a vector of probabilities for each class. Below is an example Neural Network architecture with 2 hidden layers for the Iris data which has 4 features and 3 classes.\n",
    "\n",
    "<img src='image/mlp_iris.png' style='height:300px; margin:20px auto;'>\n",
    "\n",
    "This type of architecture is generally called Deep Neural Network of Multilayer Perceptron. Notice that each node represents a vector, we can think of nodes in the hidden layers as *latent features*, as they are automatically discovered by Deep Neural Networks. Such an approach is called [representation learning], one of the benefits that Multilayer Perceptron offer.\n",
    "\n",
    "[representation learning]: https://en.wikipedia.org/wiki/Feature_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspiration\n",
    "Now we have known what a Deep Neural Network is, but what does the term *neural* implies here? This is because Neural Nets are inspired by biological neural network that constitute human brains. Artificial Neural Networks are constructed by nodes and edges, which resemble *neurons* and *synapses* in biological brains. An artificial neuron recieves signals, processes them and transmit it to other neurons. The strength of a signal between neurons is modeled by the weight of an edge.\n",
    "\n",
    "Biological nervous system in fact is much more complicated, and Artificial Neural Network is just a simple counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Now, let's construct a Neural Network for the Iris example above using TensorFlow. We use a\n",
    "<code style='font-size:13px'><a href='https://www.tensorflow.org/api_docs/python/tf/keras/Sequential'>Sequential</a></code>\n",
    "to add [layers] successively to the network. There are so many types of layer, but we will start with the most basic one, Dense (also known as Fully Connected), indicating all nodes from the previous layers connect to the next layer.\n",
    "\n",
    "The first thing to notice when constructing a Neural Network is the shape of each layer. Let's take a look at the first hidden layer: it has the shape of (None, 6). This means, this layer is a matrix with 6 columns and an unspecified number of rows. In other words, the architecture of the Neural Network is fixed, but it can adapts to any data size.\n",
    "\n",
    "Another important thing is the number of parameters (weights and biases). A large number of parameters leads to high training time and oerfitting. Here are the numbers of parameters for each layer:\n",
    "- Layer 1: $4\\times6=24$ weights and $6$ biases or $30$ in total\n",
    "- Layer 2: $6\\times6=36$ weights and $6$ biases or $42$ in total\n",
    "- Layer 3: $6\\times3=18$ weights and $3$ biases or $21$ in total\n",
    "\n",
    "[layers]: https://www.tensorflow.org/api_docs/python/tf/keras/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(units=6))\n",
    "model.add(layers.Dense(units=6))\n",
    "model.add(layers.Dense(units=3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy')\n",
    "\n",
    "model.build(input_shape=(None, 4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Activation functions\n",
    "We have already known a Deep Neural Network is simply the combination of many Logistic Regression models. But if we keep stacking up linear functions, the result is still a linear function. In other words, using multiple linear layers is the same as using a single linear layer. To go beyonds linearity, an [activation function] is added to each node. \n",
    "\n",
    "[activation function]: https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.1, 0.2,\n",
       "       0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Activation('relu')\n",
    "output = layer(np.arange(-1,1,0.1))\n",
    "output.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(units=1))\n",
    "model.compile(tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n",
    "model.build(input_shape=(None,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:19:21.207404Z",
     "iopub.status.busy": "2022-03-06T08:19:21.207195Z",
     "iopub.status.idle": "2022-03-06T08:19:21.211669Z",
     "shell.execute_reply": "2022-03-06T08:19:21.210779Z",
     "shell.execute_reply.started": "2022-03-06T08:19:21.207384Z"
    },
    "id": "1RwrL2LCk6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "y = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:26:10.244712Z",
     "iopub.status.busy": "2022-03-06T08:26:10.244453Z",
     "iopub.status.idle": "2022-03-06T08:26:10.255065Z",
     "shell.execute_reply": "2022-03-06T08:26:10.254017Z",
     "shell.execute_reply.started": "2022-03-06T08:26:10.244689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:57:23.726387Z",
     "iopub.status.busy": "2022-03-06T08:57:23.725977Z",
     "iopub.status.idle": "2022-03-06T08:57:26.001395Z",
     "shell.execute_reply": "2022-03-06T08:57:26.000600Z",
     "shell.execute_reply.started": "2022-03-06T08:57:23.726349Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03436869],\n",
       "       [ 0.9754254 ],\n",
       "       [ 1.9852195 ],\n",
       "       [ 2.9950137 ],\n",
       "       [ 4.0048075 ],\n",
       "       [ 5.0146017 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = layers.Normalization(input_shape=(1,), axis=None)\n",
    "normalizer.adapt(xs)\n",
    "\n",
    "xs = xs.reshape(-1,1)\n",
    "ys = ys.reshape(-1,1)\n",
    "model1 = keras.Sequential()\n",
    "# model1.add(normalizer)\n",
    "model1.add(layers.Dense(units=1))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy')\n",
    "\n",
    "model1.fit(xs, ys, validation_split=0.2, epochs=100, verbose=0)\n",
    "model1.predict(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
