{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d305989d-75c3-4290-a9ce-474f0690fdf0",
   "metadata": {},
   "source": [
    "# 1. Optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534108f1-9dd3-4cb3-b96b-4cd1ece68716",
   "metadata": {},
   "source": [
    "## 1.1. Problem statement\n",
    "Optimization is the problem of finding minimum or maximum of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeec38e-6fc8-419d-9caf-5ebc39596383",
   "metadata": {},
   "source": [
    "## 1.2. First-derivative test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3106fa-dd94-4ae4-a8c5-ada353dceecf",
   "metadata": {},
   "source": [
    "## 1.3. Second-derivative test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261a314-816e-42d8-b550-7463d1da983a",
   "metadata": {},
   "source": [
    "# 2. Gradient Descent\n",
    "In real life, especially when the gradient gets very complicated or is very large, mathematical methods on solving for global minimum are shown to be impossible. There are a number of computational methods have been developed in order to find extrema of a function, where [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) (GD) is one of the most pupular and is widely used in Machine Learning. This is an iterative method trying to minimize a [differentiable](https://en.wikipedia.org/wiki/Differentiable_function) function; in the context of Machine Learning, the function to be minimized is nothing but the loss function, $L(\\mathbf{w})$, where $\\mathbf{w}$ represents model parameters.\n",
    "\n",
    "There is a drawback of GD is that it is designed to find a local minimum, while we need the global minimum of the loss function. A lot of works have been proposed to tackle this problem, described in a evolutionary chart as below.\n",
    "\n",
    "<img src='image/gradient_descent_evolutionary.png' style='height:350px; margin:20px auto 20px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9dacfe-5ce1-42d7-a66f-e3253946fc22",
   "metadata": {},
   "source": [
    "## 2.1. BGD\n",
    "This section is about the most basic idea of the family, Batch/Vanilla Gradient Descent (BGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e77621-20ab-4e39-b047-901ee0810ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ArtistAnimation, PillowWriter\n",
    "plt.style.use(['seaborn', 'seaborn-whitegrid'])\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb085a-74e1-43d7-bc25-9d81fdc8e84c",
   "metadata": {},
   "source": [
    "### Analysis of derivative\n",
    "To understand the smart idea behind Gradient Descent, we first analyze the relative position between a random point to its *closest* local minimum. We have already known derivarive at a point is the representation of *instantaneous velocity*, but how about its direction? To illustrate this, let's plot the derivatives at some points of a 1-dimensional function as vectors along the $x$-axis.\n",
    "\n",
    "$$y=\\frac{1}{128}(x^4-8x^3)$$\n",
    "\n",
    "<img src='output/directional_derivative.png' style='height:300px; margin:0px auto 20px;'>\n",
    "\n",
    "In this function, there are two [critical points](<https://en.wikipedia.org/wiki/Critical_point_(mathematics)>) at $x=0$ and $x=6$, in which the first one is a [saddle point](https://en.wikipedia.org/wiki/Saddle_point) and the second one is a [local minimum](https://en.wikipedia.org/wiki/Maxima_and_minima). A very important conclusion can be drawn from this graph is that directional derivatives always *point away* from the steepest path downwards. In other words, if we move the point in the *opposite direction* of the derivative, we will end up approach a local minimum or a saddle point. Unfortunately, from the perspective of a single point on the graph and using only gradient, there is no way to regconize if there comes a saddle point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86ee47e2-9acb-489f-917e-9ea593f94b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T06:22:32.579922Z",
     "iopub.status.busy": "2022-04-29T06:22:32.579541Z",
     "iopub.status.idle": "2022-04-29T06:22:32.916149Z",
     "shell.execute_reply": "2022-04-29T06:22:32.915206Z",
     "shell.execute_reply.started": "2022-04-29T06:22:32.579889Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: 1/128 * (x**4 - 8*x**3)\n",
    "grad = lambda x: 1/32 * (x**3 - 6*x**2)\n",
    "\n",
    "left, right = -4, 10\n",
    "xData = np.linspace(left, right, 1000)\n",
    "yData = func(xData)\n",
    "\n",
    "xCritical = np.array([0, 6])\n",
    "yCritical = func(xCritical)\n",
    "\n",
    "xRed = np.array([-2])\n",
    "yRed = func(xRed)\n",
    "uRed = grad(xRed)\n",
    "vRed = np.zeros(xRed.shape)\n",
    "\n",
    "xBlue = np.array([3, 4, 7])\n",
    "yBlue = func(xBlue)\n",
    "uBlue = grad(xBlue)\n",
    "vBlue = np.zeros(xBlue.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot(xData, yData, '-', c='grey')\n",
    "ax.plot(xCritical, yCritical, 'o', c='grey')\n",
    "ax.plot(xRed, yRed, 'o', c='indianred')\n",
    "ax.quiver(xRed, yRed, uRed, vRed, color='indianred', units='xy', scale=1/2)\n",
    "ax.plot(xBlue, yBlue, 'o', c='cornflowerblue')\n",
    "ax.quiver(xBlue, yBlue, uBlue, vBlue, color='cornflowerblue', units='xy', scale=1/2)\n",
    "ax.set_xlim(left, right)\n",
    "fig.savefig('output/directional_derivative.png', dpi=500, bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d66010-5df5-4fd4-b948-3f9a1e5ff893",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "From the above analyses, an iterative method call Gradient Descent has been proposed to find local minima. This algorithm initializes an arbitrary point and update its position at each iteration $t$ using the formula:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Delta\\mathbf{w}^{(t+1)} &= -\\eta\\nabla L(\\mathbf{w}^{(t)}) \\\\\n",
    "\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} + \\Delta\\mathbf{w}^{(t+1)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Using this design, the (magnitude of) the gradient gets smaller and smaller and finally approach 0, thus the name of the algorithm. The whole process can be thought as a ball rolling down the hill. There is a coefficient named the *learning rate* (denoted $\\eta$) attached to the gradient, controls how large each step is. The value of this hyperparameter should not be either too large (making the convergence not happening) or too small (taking too long to converge). The effect of learning rate is illustrated in the following example, in which we build the algorithm from scratch to find the minimum of the function $y=x^2+5\\sin(x)$ for different values of $\\eta$.\n",
    "\n",
    "<img src='output/batch_gradient_descent.gif' style='height:250px; margin:20px auto 20px;'>\n",
    "\n",
    "The updating process can also be summarized as [learning curves](<https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)>).\n",
    "\n",
    "<img src='output/batch_gradient_descent.png' style='height:300px; margin:20px auto 20px;'>\n",
    "\n",
    "A couple of stopping conditions are also used such as tolerance (maximum magnitude of gradient) or maximum number of iterations. In the example, I use 50 iterations and set the value of tolerance to 0 to make sure all iterations are used.\n",
    "\n",
    "Finally, the Gradient Descent's formula is sometimes simplified by removing the indices of iterations:\n",
    "\n",
    "$$\\mathbf{w}\\leftarrow\\mathbf{w}-\\eta\\nabla L(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb253e0-9307-4756-a466-534ff5e03288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BatchGD(func, grad, eta, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    xList, yList = np.array(x), np.array(y)\n",
    "    \n",
    "    for i in range(nIter):\n",
    "        xDelta = - eta * grad(x)\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d311fa27-4ed2-43a4-9a68-58040ba6c9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: x**2 + 5*np.sin(x)\n",
    "grad = lambda x: 2*x + 5*np.cos(x)\n",
    "\n",
    "nIter = 30\n",
    "xInit = 5\n",
    "\n",
    "eta1 = 0.4\n",
    "eta2 = 0.1\n",
    "eta3 = 0.06\n",
    "\n",
    "frames1 = BatchGD(func, grad, eta1, nIter, xInit)\n",
    "frames2 = BatchGD(func, grad, eta2, nIter, xInit)\n",
    "frames3 = BatchGD(func, grad, eta3, nIter, xInit)\n",
    "iList = np.arange(nIter+1)\n",
    "frames = np.c_[iList, frames1, frames2, frames3]\n",
    "\n",
    "xLeft, xRight = -3, 5\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,4), sharey=True, sharex=True, constrained_layout=True)\n",
    "xGraph = np.linspace(xLeft, xRight, 1000)\n",
    "yGraph = func(xGraph)\n",
    "\n",
    "def animate(frame):\n",
    "    i, x1, y1, x2, y2, x3, y3 = frame\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(xLeft, xRight)\n",
    "    ax1.set_ylim(-4, 15)\n",
    "    ax1.set_title(f'learningRate={eta1}')\n",
    "    line1, = ax1.plot(xGraph, yGraph, c='grey')\n",
    "    point1, = ax1.plot(x1, y1, 'o', c='indianred')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.set_title(f'learningRate={eta2}')\n",
    "    line2, = ax2.plot(xGraph, yGraph, c='grey')\n",
    "    point2, = ax2.plot(x2, y2, 'o', c='indianred')\n",
    "    \n",
    "    ax3.clear()\n",
    "    ax3.set_title(f'learningRate={eta3}')\n",
    "    line3, = ax3.plot(xGraph, yGraph, c='grey')\n",
    "    point3, = ax3.plot(x3, y3, 'o', c='indianred')\n",
    "    \n",
    "    fig.suptitle(f'Iteration {i:.0f}/{nIter}', size=14)\n",
    "    \n",
    "    return line1, point1, line2, point2, line3, point3\n",
    "\n",
    "gif = FuncAnimation(fig, animate, frames, interval=200, blit=False, repeat=True)\n",
    "plt.close(fig)\n",
    "\n",
    "path = 'output/batch_gradient_descent.gif'\n",
    "gif.save(path, dpi=300, writer=PillowWriter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1dc8c1c-9831-4a7d-b54e-f7bd922816a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T07:56:34.336312Z",
     "iopub.status.busy": "2022-05-02T07:56:34.336028Z",
     "iopub.status.idle": "2022-05-02T07:56:34.898124Z",
     "shell.execute_reply": "2022-05-02T07:56:34.897510Z",
     "shell.execute_reply.started": "2022-05-02T07:56:34.336291Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: x**2 + 5*np.sin(x)\n",
    "grad = lambda x: 2*x + 5*np.cos(x)\n",
    "\n",
    "nIter = 50\n",
    "xInit = 5\n",
    "etaList = [0.4, 0.1, 0.06]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for eta in etaList:\n",
    "    xList, yList = BatchGD(func, grad, eta, nIter, xInit).T\n",
    "    iList = np.arange(nIter+1)\n",
    "    ax.plot(iList, yList, label=f'eta={eta}')\n",
    "ax.legend()\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('value')\n",
    "\n",
    "fig.savefig('output/batch_gradient_descent.png', dpi=500, bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25033abf-2181-4f54-a7aa-3ab25f8303bc",
   "metadata": {},
   "source": [
    "## 2.2. SGD\n",
    "In this section, we talk about some drawbacks of BGD in practice: (1) its heavy dependence on the intial point, (2) the capability of online learning, (3) the memory cost and how [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) comes to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f91f87-e349-4ecc-899d-fa979968db97",
   "metadata": {},
   "source": [
    "### Stochastic behaviour\n",
    "This Gradient Descent's variant only considers a part of data (mini-batch, or batch for short) instead of the whole dataset (full-batch) in each iteration to compute the gradient. The term *stochastic* means we add something *random*, *non-deterministic* into the algorithm. At first glance, a common sense is that using a part of data is less accurate, but it turns out SGD is amazingly efficient in practice. Let's analyze the advantages of SGD over BGD.\n",
    "- SGD makes data in each iteration small enough so that it can be loaded into RAM with ease, this also reduces the computation cost significantly.\n",
    "- Randomness in SGD works as a regularization mechanic, some sort of trade-off between exploration and exploitation. In short-term, noisy steps can lead the ball away from local minima or saddle points; while in long-term, the ball still tends to finish in a valley bottom. For BGD, the ball goes straight to the local minimum; this behaviour is deterministic and thus has no exploration.\n",
    "- SGD enables *online learning*, which is a very important feature when implementing in practice. When there are new data, SGD treats them as a number of batches and updates to the current model easily, without re-computing gradients for the entire dataset.\n",
    "\n",
    "As steps in SGD are very noisy, we need to update more frequently than BGD to reach *long-term* state. This leads to the idea of using more than one epoch (an epoch is a pass over all data samples), which will be described in the next part. Nowadays, the SGD algorithm using the epoch concept is implemented in many modern ML/DL frameworks. Later improved techniques are also developed based on this implementation; however, I still use BGD to make things as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059e78d-bcff-4185-b535-946170ddcaf9",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "*Input*\n",
    "- A dataset $\\mathcal{D}$ having $N$ samples\n",
    "- A loss function $L(\\mathbf{w})$ and its gradient $\\nabla{L}$\n",
    "- A learning rate, $\\eta$\n",
    "- A batch size, $B$\n",
    "- A number of epochs, $E$\n",
    "\n",
    "*Step 1*. Calculate the number of batches $T=\\lceil N/B\\rceil$.\n",
    "\n",
    "*Step 2*. Initialize model parameters $\\mathbf{w}$ randomly.\n",
    "\n",
    "*Step 3*. For $e=1,2,\\dots,E$:\n",
    "- Shuffle the training set $\\mathcal{D}$ to renew batches.\n",
    "- Divide $\\mathcal{D}$ into $T$ batches, denoted $\\mathcal{B}_1,\\mathcal{B}_2,\\dots,\\mathcal{B}_T$. Each batch has the size of $B$ and the last batch may have less than $B$ samples.\n",
    "- For $t=1,2,\\dots,T$:\n",
    "    - Compute the gradient $\\nabla{L(\\mathbf{w})}$ for batch $\\mathcal{B}_t$\n",
    "    - Compute the step size by multiplying the learning rate and the gradient\n",
    "    - Update the position using the rule: $\\mathbf{w}\\leftarrow\\mathbf{w}-\\eta\\,\\nabla{L(\\mathbf{w})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b75110-0acf-4a23-9f5f-44be5fb46754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T17:39:26.551771Z",
     "iopub.status.busy": "2022-04-17T17:39:26.549567Z",
     "iopub.status.idle": "2022-04-17T17:39:26.702863Z",
     "shell.execute_reply": "2022-04-17T17:39:26.701242Z",
     "shell.execute_reply.started": "2022-04-17T17:39:26.551376Z"
    },
    "tags": []
   },
   "source": [
    "## 2.3. Momentum\n",
    "As far as we know, GD works as a ball rolling down the hill and stops in a valley bottom. However, our ball will stuck in local minima most of the time, then we need some [acceleration](https://en.wikipedia.org/wiki/Acceleration) to helps it cross these traps. A Momentum term has been introduced to extend the GD's update rule as follow:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Delta\\mathbf{w}^{(t+1)} &= \\gamma\\Delta\\mathbf{w}^{(t)}-\\eta\\nabla L(\\mathbf{w}^{(t)}) \\\\\n",
    "\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)}+\\Delta\\mathbf{w}^{(t+1)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "In each step, the ball not only moves downwards as normal GD, but also accumulates [momentum](https://en.wikipedia.org/wiki/Momentum) from all previous steps:\n",
    "\n",
    "$$\\Delta\\mathbf{w}^{(T)}=-\\eta\\sum_{t=1}^{T}{\\gamma^t\\nabla L(\\mathbf{w}^{(t)})}$$\n",
    "\n",
    "The amount of information memorized from the previous step is controled by a coefficient, $\\gamma$. The value of this hyperparameter is set $0<\\gamma<1$, typically $0.9$, forcing earlier steps to have less effect. Visually, the motion of the ball is now more realistic, as it seems to carry [inertia](https://en.wikipedia.org/wiki/Inertia).\n",
    "\n",
    "<img src='output/momentum_gradient_descent.gif' style='height:250px; margin:20px auto 20px;'>\n",
    "\n",
    "The benefits of using Momentum includes:\n",
    "- Momentum can help escaping local minima and saddle points\n",
    "- Momentum accelerates the ball so that it moves faster towards the minima\n",
    "- When implemented in SGD, Momentum dampens the *oscillations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadc79c0-4ef8-4e63-be02-5b0f6ca271b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ArtistAnimation, PillowWriter\n",
    "plt.style.use(['seaborn', 'seaborn-whitegrid'])\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f82940-66a4-4165-8b59-926c571af93d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:11:08.959476Z",
     "iopub.status.busy": "2022-05-03T03:11:08.959204Z",
     "iopub.status.idle": "2022-05-03T03:11:09.038043Z",
     "shell.execute_reply": "2022-05-03T03:11:09.036799Z",
     "shell.execute_reply.started": "2022-05-03T03:11:08.959451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BatchGD(func, grad, eta, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    xList, yList = np.array(x), np.array(y)\n",
    "    \n",
    "    for i in range(nIter):\n",
    "        xDelta = - eta * grad(x)\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]\n",
    "\n",
    "def MomentumGD(func, grad, eta, gamma, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    xDelta = 0\n",
    "    xList, yList = np.array(x), np.array(y)\n",
    "    \n",
    "    for i in range(nIter):\n",
    "        xDelta = gamma * xDelta - eta * grad(x)\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e998b01-564c-41f9-bcab-f4846ef9d742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T07:24:40.884542Z",
     "iopub.status.busy": "2022-05-02T07:24:40.884218Z",
     "iopub.status.idle": "2022-05-02T07:24:56.399252Z",
     "shell.execute_reply": "2022-05-02T07:24:56.398472Z",
     "shell.execute_reply.started": "2022-05-02T07:24:40.884510Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: 1/12 * (3*x**4 - 16*x**3 + 18*x**2)\n",
    "grad = lambda x: x**3 - 4*x**2 + 3*x\n",
    "\n",
    "nIter = 50\n",
    "xInit = -1.5\n",
    "eta = 0.08\n",
    "gamma = 0.8\n",
    "\n",
    "frames1 = BatchGD(func, grad, eta, nIter, xInit)\n",
    "frames2 = MomentumGD(func, grad, eta, gamma, nIter, xInit)\n",
    "iList = np.arange(nIter+1)\n",
    "frames = np.c_[iList, frames1, frames2]\n",
    "\n",
    "xLeft, xRight = -2, 4\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8,4), sharey=True, sharex=True, constrained_layout=True)\n",
    "xGraph = np.linspace(xLeft, xRight, 1000)\n",
    "yGraph = func(xGraph)\n",
    "\n",
    "def animate(frame):\n",
    "    i, x1, y1, x2, y2 = frame\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(xLeft, xRight)\n",
    "    ax1.set_ylim(-4, 15)\n",
    "    ax1.set_title(f'learningRate={eta}')\n",
    "    line1, = ax1.plot(xGraph, yGraph, c='grey')\n",
    "    point1, = ax1.plot(x1, y1, 'o', c='indianred')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.set_title(f'learningRate={eta}, momentum={gamma}')\n",
    "    line2, = ax2.plot(xGraph, yGraph, c='grey')\n",
    "    point2, = ax2.plot(x2, y2, 'o', c='indianred')\n",
    "    \n",
    "    fig.suptitle(f'Iteration {i:.0f}/{nIter}', size=14)\n",
    "    \n",
    "    return line1, point1, line2, point2\n",
    "\n",
    "gif = FuncAnimation(fig, animate, frames, interval=200, blit=False, repeat=True)\n",
    "plt.close(fig)\n",
    "\n",
    "path = 'output/momentum_gradient_descent.gif'\n",
    "gif.save(path, dpi=300, writer=PillowWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fc90f-22ed-405a-a6bd-3299e3084cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T17:39:26.551771Z",
     "iopub.status.busy": "2022-04-17T17:39:26.549567Z",
     "iopub.status.idle": "2022-04-17T17:39:26.702863Z",
     "shell.execute_reply": "2022-04-17T17:39:26.701242Z",
     "shell.execute_reply.started": "2022-04-17T17:39:26.551376Z"
    },
    "tags": []
   },
   "source": [
    "## 2.4. NAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9aa22-4b88-49e6-922d-ac624369ddd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T17:39:26.551771Z",
     "iopub.status.busy": "2022-04-17T17:39:26.549567Z",
     "iopub.status.idle": "2022-04-17T17:39:26.702863Z",
     "shell.execute_reply": "2022-04-17T17:39:26.701242Z",
     "shell.execute_reply.started": "2022-04-17T17:39:26.551376Z"
    },
    "tags": []
   },
   "source": [
    "## 2.5. AdaGrad\n",
    "\n",
    "The optimizers above remain one learning rate constant through training while AdaGrad adapts learning rate to the parameters, performing low learning rates for parameters associated with frequently features, and higher learning rates for parameters associated with infrequent features. AdaGrad suitables for dealing with sparse data, the learning rate will be updated after each iteration.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g_t &= \\nabla L(\\mathbf{w}^{(t)})\\\\\n",
    "\\Delta\\mathbf{w}^{(t+1)} &= -\\eta_{(t+1)}g_t \\\\\n",
    "\\eta_{(t+1)} &= \\frac{\\eta}{\\sqrt{\\sum_{t=1}^{T}{g_t^2} + \\epsilon}} \\\\ \n",
    "\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)}+\\Delta\\mathbf{w}^{(t+1)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The new learning rate $\\eta_{t+1}$ will be affected by accumulate gradient of all previous step so that it will get smaller after each interations and can lead to slow convergence, so the initiation value of learning rate should be high, typically $\\eta=1.0$ (the $\\epsilon$ is very small positive which added to prevent divide by zero error)\n",
    "\n",
    "<img src='output/adagrad.gif' style='height:250px; margin:20px auto 20px;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985464e9-1708-4246-b010-d5ad4de5a1ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T11:01:33.601426Z",
     "iopub.status.busy": "2022-07-18T11:01:33.600767Z",
     "iopub.status.idle": "2022-07-18T11:01:34.611863Z",
     "shell.execute_reply": "2022-07-18T11:01:34.611015Z",
     "shell.execute_reply.started": "2022-07-18T11:01:33.601305Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ArtistAnimation, PillowWriter\n",
    "plt.style.use(['seaborn', 'seaborn-whitegrid'])\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0590c845-d505-4cd0-91c4-34a40023ca1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T11:01:47.824329Z",
     "iopub.status.busy": "2022-07-18T11:01:47.823559Z",
     "iopub.status.idle": "2022-07-18T11:01:47.832069Z",
     "shell.execute_reply": "2022-07-18T11:01:47.830880Z",
     "shell.execute_reply.started": "2022-07-18T11:01:47.824280Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AdaGrad(func, grad, eta, eps, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    gradInit = grad(x)\n",
    "    xList, yList, gradList = np.array(x), np.array(y), np.array(gradInit)\n",
    "    \n",
    "    for i in range(nIter):\n",
    "        etaAdj = eta / np.sqrt(np.sum(gradList**2) + eps)\n",
    "        xDelta = - etaAdj * grad(x)\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        gradList = np.append(gradList,grad(x))\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce48c143-e298-4957-9d4b-56761512c555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T11:01:54.344714Z",
     "iopub.status.busy": "2022-07-18T11:01:54.344222Z",
     "iopub.status.idle": "2022-07-18T11:02:17.388587Z",
     "shell.execute_reply": "2022-07-18T11:02:17.387772Z",
     "shell.execute_reply.started": "2022-07-18T11:01:54.344674Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: x**2 + 5*np.sin(x)\n",
    "grad = lambda x: 2*x + 5*np.cos(x)\n",
    "\n",
    "nIter = 50\n",
    "xInit = 5\n",
    "\n",
    "eta1 = .1\n",
    "eta2 = .8\n",
    "eta3 = 1.0\n",
    "eps = 1e-8\n",
    "\n",
    "frames1 = AdaGrad(func, grad, eta1, eps, nIter, xInit)\n",
    "frames2 = AdaGrad(func, grad, eta2, eps, nIter, xInit)\n",
    "frames3 = AdaGrad(func, grad, eta3, eps, nIter, xInit)\n",
    "iList = np.arange(nIter+1)\n",
    "frames = np.c_[iList, frames1, frames2, frames3]\n",
    "\n",
    "xLeft, xRight = -3, 5\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,4), sharey=True, sharex=True, constrained_layout=True)\n",
    "xGraph = np.linspace(xLeft, xRight, 1000)\n",
    "yGraph = func(xGraph)\n",
    "\n",
    "def animate(frame):\n",
    "    i, x1, y1, x2, y2, x3, y3 = frame\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(xLeft, xRight)\n",
    "    ax1.set_ylim(-4, 15)\n",
    "    ax1.set_title(f'learningRate={eta1}')\n",
    "    line1, = ax1.plot(xGraph, yGraph, c='grey')\n",
    "    point1, = ax1.plot(x1, y1, 'o', c='indianred')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.set_title(f'learningRate={eta2}')\n",
    "    line2, = ax2.plot(xGraph, yGraph, c='grey')\n",
    "    point2, = ax2.plot(x2, y2, 'o', c='indianred')\n",
    "    \n",
    "    ax3.clear()\n",
    "    ax3.set_title(f'learningRate={eta3}')\n",
    "    line3, = ax3.plot(xGraph, yGraph, c='grey')\n",
    "    point3, = ax3.plot(x3, y3, 'o', c='indianred')\n",
    "    \n",
    "    fig.suptitle(f'Iteration {i:.0f}/{nIter}', size=14)\n",
    "    \n",
    "    return line1, point1, line2, point2, line3, point3\n",
    "\n",
    "gif = FuncAnimation(fig, animate, frames, interval=200, blit=False, repeat=True)\n",
    "plt.close(fig)\n",
    "\n",
    "path = 'output/ada_gradient_descent.gif'\n",
    "gif.save(path, dpi=300, writer=PillowWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c7515-e1d7-4305-bdc6-ef13c5f50944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T17:39:26.551771Z",
     "iopub.status.busy": "2022-04-17T17:39:26.549567Z",
     "iopub.status.idle": "2022-04-17T17:39:26.702863Z",
     "shell.execute_reply": "2022-04-17T17:39:26.701242Z",
     "shell.execute_reply.started": "2022-04-17T17:39:26.551376Z"
    },
    "tags": []
   },
   "source": [
    "## 2.6. AdaDelta\n",
    "\n",
    "AdaDelta is the update version of AdaGrad which try to solve the radically diminishing learning rates problem. The main idea is instead of summing all squared gradient up to $t$, the algorithm take accumulate the sum of squared gradients over a restricted time window. This can be archived by using [Exponentially Weighted Averages](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average). Call $S_{w_t}$ is EMA of squared gradient at loop $t$, we have:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Delta\\mathbf{w}^{(t+1)} &= -\\eta_{(t+1)}g_t \\\\\n",
    "\\eta_{(t+1)} &= \\frac{\\eta}{\\sqrt{S_{w_t} + \\epsilon}} \\\\ \n",
    "S_{w_t} &= \\beta S_{w_{t-1}} + (1-\\beta) g_{t-1}^2\\\\ \n",
    "\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)}+\\Delta\\mathbf{w}^{(t+1)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "<img src='output/adadelta.gif' style='height:250px; margin:20px auto 20px;'>\n",
    "\n",
    "The number of pass gradient which $S_w$ uses depend on the amount of $\\beta$ with $n_{g_t}\\approx \\dfrac{1}{1-\\beta}$, typically choosing $\\beta$ between 0.9 and 0.98. For example, if $\\beta = 0.9$, which mean we use 10 pass gradient to calculate $S_{w_t}$ - this can explane how we user $\\beta$ to control the learning rate. With the larger value of $\\beta$, the learning rate adapt more slowly. As same as AdaGrad, we should choose high value of learning rate in the range 0.1 to 0.5, in case low init learning rate can lead to slow convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e42a9-2b3e-41d6-8280-b4793bd735c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T17:39:26.551771Z",
     "iopub.status.busy": "2022-04-17T17:39:26.549567Z",
     "iopub.status.idle": "2022-04-17T17:39:26.702863Z",
     "shell.execute_reply": "2022-04-17T17:39:26.701242Z",
     "shell.execute_reply.started": "2022-04-17T17:39:26.551376Z"
    },
    "tags": []
   },
   "source": [
    "### AdaDelta (New version)\n",
    "\n",
    "AdaDelta is the update version of AdaGrad which try to solve the radically diminishing learning rates problem. The main idea is instead of summing all squared gradient up to $t$, the algorithm take accumulate the sum of squared gradients over a restricted time window. This can be archived by using [Exponentially Weighted Moving Averages](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average). Secondly, instead of using init learning rate, AdaDelta using sum squared of $\\Delta\\mathbf{w}$ in order to correct the unit of function result to the parameter. Call $\\mathbf{RMS_{\\Delta\\mathbf{w_{t-1}}}}$ is *EWMA of squared delta* (also called *Root mean squared*) and $\\mathbf{RMS_{g_t}}$ is *EWMA of square gradient* at loop $t$ , we have:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Delta\\mathbf{w}^{(t+1)} &= - \\frac{\\mathbf{RMS_{\\Delta\\mathbf{w_{t-1}}}}}{\\mathbf{RMS_{g_t}}} g_t \\\\\n",
    "\\mathbf{RMS_{\\Delta\\mathbf{w_{t-1}}}} &= \\sqrt{D_{t-1} + \\epsilon} = \\sqrt{\\gamma D_{t-2} + (1-\\gamma) (\\Delta\\mathbf{w}^{(t-1)})^2 + \\epsilon} \\\\\n",
    "\\mathbf{RMS_{g_t}} &= \\sqrt{S_{w_t} + \\epsilon} = \\sqrt{\\gamma S_{w_{t-1}} + (1-\\gamma) g_{t}^2 + \\epsilon}\\\\ \n",
    "\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)}+\\Delta\\mathbf{w}^{(t+1)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The number of pass gradient which $\\mathbf{RMS_{g_t}}$ uses depend on the amount of $\\gamma$ with $n_{g_t}\\approx \\frac{1}{1-\\gamma}$, typically choosing $\\gamma$ between 0.9 and 0.98. For example, if $\\gamma = 0.9$, which mean we use 10 pass gradient to calculate $\\mathbf{RMS_{g_t}}$ - this can explane how we user $\\gamma$ to control the learning rate. With the larger value of $\\gamma$, the learning rate adapt more slowly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44ff235-be4c-4d55-8f18-7cb1c0236234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T11:04:02.799760Z",
     "iopub.status.busy": "2022-07-18T11:04:02.799169Z",
     "iopub.status.idle": "2022-07-18T11:04:02.816956Z",
     "shell.execute_reply": "2022-07-18T11:04:02.815973Z",
     "shell.execute_reply.started": "2022-07-18T11:04:02.799716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ArtistAnimation, PillowWriter\n",
    "plt.style.use(['seaborn', 'seaborn-whitegrid'])\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa4c60c-ee47-42ee-9828-b1a759b4f038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T11:04:03.977067Z",
     "iopub.status.busy": "2022-07-18T11:04:03.976488Z",
     "iopub.status.idle": "2022-07-18T11:04:03.985782Z",
     "shell.execute_reply": "2022-07-18T11:04:03.984233Z",
     "shell.execute_reply.started": "2022-07-18T11:04:03.977024Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AdaDelta(func, grad, eta, eps, beta, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    exp = 0\n",
    "    xList, yList = np.array(x), np.array(y)\n",
    "    \n",
    "    for i in range(nIter):\n",
    "        exp = beta * exp + (1-beta) * grad(x)**2\n",
    "        etaAdj = eta / np.sqrt(exp + eps)\n",
    "        xDelta = - etaAdj * grad(x)\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0a3ac84-4bff-4ea7-bf64-502646be7961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new class for AdaDelta\n",
    "def AdaDelta(func, grad, gamma, eps, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    Sw = 0\n",
    "    Dw = 0\n",
    "    xList, yList, etaList = np.array(x), np.array(y), np.array(eta)\n",
    "    \n",
    "    for i in range(nIter):\n",
    "        Sw = gamma * Sw + (1-gamma) * grad(x)**2\n",
    "        xDelta = - np.sqrt(Dw + eps)/np.sqrt(Sw + eps) * grad(x)\n",
    "        Dw = gamma * Dw + (1-gamma) * xDelta**2\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        # eta = np.sqrt(Dw + eps)/np.sqrt(Sw + eps)\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        # etaList = np.append(etaList, eta)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8877631-ee8a-4ceb-961d-eb87c54aa06e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T11:04:05.985890Z",
     "iopub.status.busy": "2022-07-18T11:04:05.985535Z",
     "iopub.status.idle": "2022-07-18T11:04:05.990250Z",
     "shell.execute_reply": "2022-07-18T11:04:05.989281Z",
     "shell.execute_reply.started": "2022-07-18T11:04:05.985864Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: x**2 + 5*np.sin(x)\n",
    "grad = lambda x: 2*x + 5*np.cos(x)\n",
    "\n",
    "nIter = 60\n",
    "xInit = 5\n",
    "\n",
    "eta = .1\n",
    "eps = 1e-8\n",
    "beta1 = .5\n",
    "beta2 = .9\n",
    "beta3 = .98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcb8d43-6a97-4ca7-8972-60243891c202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T11:04:07.458864Z",
     "iopub.status.busy": "2022-07-18T11:04:07.458383Z",
     "iopub.status.idle": "2022-07-18T11:04:35.637566Z",
     "shell.execute_reply": "2022-07-18T11:04:35.636693Z",
     "shell.execute_reply.started": "2022-07-18T11:04:07.458835Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames1 = AdaDelta(func, grad, eta, beta1, eps, nIter, xInit)\n",
    "frames2 = AdaDelta(func, grad, eta, beta2, eps, nIter, xInit)\n",
    "frames3 = AdaDelta(func, grad, eta, beta3, eps, nIter, xInit)\n",
    "iList = np.arange(nIter+1)\n",
    "frames = np.c_[iList, frames1, frames2, frames3]\n",
    "\n",
    "xLeft, xRight = -3, 5\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,4), sharey=True, sharex=True, constrained_layout=True)\n",
    "xGraph = np.linspace(xLeft, xRight, 1000)\n",
    "yGraph = func(xGraph)\n",
    "\n",
    "def animate(frame):\n",
    "    i, x1, y1, x2, y2, x3, y3 = frame\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(xLeft, xRight)\n",
    "    ax1.set_ylim(-4, 15)\n",
    "    ax1.set_title(f'beta={beta1}')\n",
    "    line1, = ax1.plot(xGraph, yGraph, c='grey')\n",
    "    point1, = ax1.plot(x1, y1, 'o', c='indianred')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.set_title(f'beta={beta2}')\n",
    "    line2, = ax2.plot(xGraph, yGraph, c='grey')\n",
    "    point2, = ax2.plot(x2, y2, 'o', c='indianred')\n",
    "    \n",
    "    ax3.clear()\n",
    "    ax3.set_title(f'beta={beta3}')\n",
    "    line3, = ax3.plot(xGraph, yGraph, c='grey')\n",
    "    point3, = ax3.plot(x3, y3, 'o', c='indianred')\n",
    "    \n",
    "    fig.suptitle(f'Iteration {i:.0f}/{nIter}', size=14)\n",
    "    \n",
    "    return line1, point1, line2, point2, line3, point3\n",
    "\n",
    "gif = FuncAnimation(fig, animate, frames, interval=200, blit=False, repeat=True)\n",
    "plt.close(fig)\n",
    "\n",
    "path = 'output/adadelta.gif'\n",
    "gif.save(path, dpi=300, writer=PillowWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88276259-cf09-41f0-ad53-2f699377514c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T17:39:26.551771Z",
     "iopub.status.busy": "2022-04-17T17:39:26.549567Z",
     "iopub.status.idle": "2022-04-17T17:39:26.702863Z",
     "shell.execute_reply": "2022-04-17T17:39:26.701242Z",
     "shell.execute_reply.started": "2022-04-17T17:39:26.551376Z"
    },
    "tags": []
   },
   "source": [
    "## 2.7. RMSprop\n",
    "\n",
    "RMSprop and AdaDelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. The difference between RMSprop and AdaDelta is RMSprop still need learning rate at first of the training.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Delta\\mathbf{w}^{(t+1)} &= - \\frac{\\eta}{\\mathbf{RMS_{g_t}}} g_t \\\\\n",
    "\\mathbf{RMS_{g_t}} &= \\sqrt{S_{w_t} + \\epsilon} = \\sqrt{\\gamma S_{w_{t-1}} + (1-\\gamma) g_{t}^2 + \\epsilon}\\\\ \n",
    "\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)}+\\Delta\\mathbf{w}^{(t+1)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "<img src='output/rmsprop.gif' style='height:250px; margin:20px auto 20px;'>\n",
    "\n",
    "As same as AdaDelta, we should choose $\\gamma$ between 0.9 and 0.98, the learning rate should be high in range 0.1 to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8feb73c-6796-4568-b63e-7e66d1672ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:58:15.170025Z",
     "iopub.status.busy": "2022-07-31T23:58:15.169620Z",
     "iopub.status.idle": "2022-07-31T23:58:16.099039Z",
     "shell.execute_reply": "2022-07-31T23:58:16.098035Z",
     "shell.execute_reply.started": "2022-07-31T23:58:15.169961Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ArtistAnimation, PillowWriter\n",
    "plt.style.use(['seaborn', 'seaborn-whitegrid'])\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c9b51a-97a5-4976-b234-11bd40fe0721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:58:17.408765Z",
     "iopub.status.busy": "2022-07-31T23:58:17.408371Z",
     "iopub.status.idle": "2022-07-31T23:58:17.415455Z",
     "shell.execute_reply": "2022-07-31T23:58:17.414266Z",
     "shell.execute_reply.started": "2022-07-31T23:58:17.408738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RMSprop(func, grad, eta, eps, gamma, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    exp = 0\n",
    "    xList, yList = np.array(x), np.array(y)\n",
    "    \n",
    "    for i in range(nIter):\n",
    "        exp = gamma * exp + (1-gamma) * grad(x)**2\n",
    "        etaAdj = eta / np.sqrt(exp + eps)\n",
    "        xDelta = - etaAdj * grad(x)\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630838b2-cfb7-44f5-b1c2-6f3ae7cb1e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:58:19.411786Z",
     "iopub.status.busy": "2022-07-31T23:58:19.411426Z",
     "iopub.status.idle": "2022-07-31T23:58:19.416143Z",
     "shell.execute_reply": "2022-07-31T23:58:19.415025Z",
     "shell.execute_reply.started": "2022-07-31T23:58:19.411762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: x**2 + 5*np.sin(x)\n",
    "grad = lambda x: 2*x + 5*np.cos(x)\n",
    "\n",
    "nIter = 65\n",
    "xInit = 5\n",
    "\n",
    "eta = .1\n",
    "eps = 1e-8\n",
    "gamma1 = .5\n",
    "gamma2 = .9\n",
    "gamma3 = .98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0609fed3-cdc3-4b9d-8983-a328b74c3e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:58:21.366365Z",
     "iopub.status.busy": "2022-07-31T23:58:21.366054Z",
     "iopub.status.idle": "2022-07-31T23:58:49.565793Z",
     "shell.execute_reply": "2022-07-31T23:58:49.564907Z",
     "shell.execute_reply.started": "2022-07-31T23:58:21.366345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames1 = RMSprop(func, grad, eta, gamma1, eps, nIter, xInit)\n",
    "frames2 = RMSprop(func, grad, eta, gamma2, eps, nIter, xInit)\n",
    "frames3 = RMSprop(func, grad, eta, gamma3, eps, nIter, xInit)\n",
    "iList = np.arange(nIter+1)\n",
    "frames = np.c_[iList, frames1, frames2, frames3]\n",
    "\n",
    "xLeft, xRight = -3, 5\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,4), sharey=True, sharex=True, constrained_layout=True)\n",
    "xGraph = np.linspace(xLeft, xRight, 1000)\n",
    "yGraph = func(xGraph)\n",
    "\n",
    "def animate(frame):\n",
    "    i, x1, y1, x2, y2, x3, y3 = frame\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(xLeft, xRight)\n",
    "    ax1.set_ylim(-4, 20)\n",
    "    ax1.set_title(f'gamma={gamma1}')\n",
    "    line1, = ax1.plot(xGraph, yGraph, c='grey')\n",
    "    point1, = ax1.plot(x1, y1, 'o', c='indianred')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.set_title(f'gamma={gamma2}')\n",
    "    line2, = ax2.plot(xGraph, yGraph, c='grey')\n",
    "    point2, = ax2.plot(x2, y2, 'o', c='indianred')\n",
    "    \n",
    "    ax3.clear()\n",
    "    ax3.set_title(f'gamma={gamma3}')\n",
    "    line3, = ax3.plot(xGraph, yGraph, c='grey')\n",
    "    point3, = ax3.plot(x3, y3, 'o', c='indianred')\n",
    "    \n",
    "    fig.suptitle(f'Iteration {i:.0f}/{nIter}', size=14)\n",
    "    \n",
    "    return line1, point1, line2, point2, line3, point3\n",
    "\n",
    "gif = FuncAnimation(fig, animate, frames, interval=200, blit=False, repeat=True)\n",
    "plt.close(fig)\n",
    "\n",
    "path = 'output/rmsprop.gif'\n",
    "gif.save(path, dpi=300, writer=PillowWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398d32d-5200-49fa-80e6-47a528262cbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T17:39:26.551771Z",
     "iopub.status.busy": "2022-04-17T17:39:26.549567Z",
     "iopub.status.idle": "2022-04-17T17:39:26.702863Z",
     "shell.execute_reply": "2022-04-17T17:39:26.701242Z",
     "shell.execute_reply.started": "2022-04-17T17:39:26.551376Z"
    },
    "tags": []
   },
   "source": [
    "## 2.8. Adam\n",
    "\n",
    "Adam is the combine of momentum and RMSprop. It uses both exponentially moving averages of $m$ pass gradient (like momentum) and exponentially moving averages of $v$ pass squared gradient (like RMSprop). According to the author of Adam, the inititialize value of $m_t$ and $v_t$ are 0 so they are biased towards 0, especially with very large $\\beta_1$ and $\\beta_2$. Algorithm fix these biases by computing bias-corrected first and second moment estimates:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Delta\\mathbf{w}^{(t+1)} &= - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon} \\hat{m}_t \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} = \\frac{\\beta_1 m_{t-1} + (1-\\beta_1)g_t }{1-\\beta_1^t} \\\\ \n",
    "\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} = \\frac{\\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 }{1-\\beta_2^t} \\\\\n",
    "\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)}+\\Delta\\mathbf{w}^{(t+1)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "<img src='output/adam.gif' style='height:250px; margin:20px auto 20px;'>\n",
    "\n",
    "Default value for $\\beta_1$ is 0.9, $\\beta_2$ is 0.999 and $\\epsilon$ is $10^{-8}$. Because $\\eta$ is constant value in the process, if we choose high $\\eta$, the convergence wil be faster than small init $\\eta$. Adam has advantage of momentum and RMSprop which can works well with sparse data, has fast learning time and can work well in online and non stationary setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffecb02-a078-44f2-9b5f-3eb0af351b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:59:02.252169Z",
     "iopub.status.busy": "2022-07-31T23:59:02.251832Z",
     "iopub.status.idle": "2022-07-31T23:59:02.264357Z",
     "shell.execute_reply": "2022-07-31T23:59:02.263415Z",
     "shell.execute_reply.started": "2022-07-31T23:59:02.252147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ArtistAnimation, PillowWriter\n",
    "plt.style.use(['seaborn', 'seaborn-whitegrid'])\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a925962-1b90-44c7-b432-ea9b7474cfbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:59:03.634587Z",
     "iopub.status.busy": "2022-07-31T23:59:03.634177Z",
     "iopub.status.idle": "2022-07-31T23:59:03.641915Z",
     "shell.execute_reply": "2022-07-31T23:59:03.640844Z",
     "shell.execute_reply.started": "2022-07-31T23:59:03.634558Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Adam(func, grad, eta, eps, beta1, beta2, nIter, xInit, tol=0):\n",
    "    x = xInit\n",
    "    y = func(x)\n",
    "    m = 0\n",
    "    v = 0\n",
    "    xList, yList = np.array(x), np.array(y)\n",
    "    \n",
    "    for i in range(1,nIter):\n",
    "        m = beta1 * m + (1-beta1)*grad(x)\n",
    "        v = beta2 * v + (1-beta2)*grad(x)**2\n",
    "        m_hat = m/(1-beta1**i)\n",
    "        v_hat = v/(1-beta2**i)\n",
    "        xDelta = -eta * m_hat/ (np.sqrt(v_hat) + eps)\n",
    "        x = x + xDelta\n",
    "        y = func(x)\n",
    "        xList = np.append(xList, x)\n",
    "        yList = np.append(yList, y)\n",
    "        if np.abs(grad(x)) < tol: break\n",
    "    \n",
    "    return np.c_[xList, yList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4fb0f9-a4d3-4354-a942-1c21532e76ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:59:04.787873Z",
     "iopub.status.busy": "2022-07-31T23:59:04.787451Z",
     "iopub.status.idle": "2022-07-31T23:59:04.793258Z",
     "shell.execute_reply": "2022-07-31T23:59:04.792311Z",
     "shell.execute_reply.started": "2022-07-31T23:59:04.787842Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = lambda x: x**2 + 5*np.sin(x)\n",
    "grad = lambda x: 2*x + 5*np.cos(x)\n",
    "\n",
    "nIter = 50\n",
    "xInit = 5\n",
    "\n",
    "eta1 = .1\n",
    "eta2 = .3\n",
    "eta3 = .5\n",
    "eps = 1e-8\n",
    "beta1 = .9\n",
    "beta2 = .999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "878c5133-16f1-4fa3-b56d-d073b7cd354c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T23:59:06.189205Z",
     "iopub.status.busy": "2022-07-31T23:59:06.188772Z",
     "iopub.status.idle": "2022-07-31T23:59:28.072314Z",
     "shell.execute_reply": "2022-07-31T23:59:28.071553Z",
     "shell.execute_reply.started": "2022-07-31T23:59:06.189174Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames1 = Adam(func, grad, eta1, eps, beta1, beta2, nIter, xInit)\n",
    "frames2 = Adam(func, grad, eta2, eps, beta1, beta2, nIter, xInit)\n",
    "frames3 = Adam(func, grad, eta3, eps, beta1, beta2, nIter, xInit)\n",
    "iList = np.arange(1,nIter+1)\n",
    "frames = np.c_[iList, frames1, frames2, frames3]\n",
    "\n",
    "xLeft, xRight = -3, 5\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,4), sharey=True, sharex=True, constrained_layout=True)\n",
    "xGraph = np.linspace(xLeft, xRight, 1000)\n",
    "yGraph = func(xGraph)\n",
    "\n",
    "def animate(frame):\n",
    "    i, x1, y1, x2, y2, x3, y3 = frame\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(xLeft, xRight)\n",
    "    ax1.set_ylim(-4, 20)\n",
    "    ax1.set_title(f'eta={eta1}')\n",
    "    line1, = ax1.plot(xGraph, yGraph, c='grey')\n",
    "    point1, = ax1.plot(x1, y1, 'o', c='indianred')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.set_title(f'eta={eta2}')\n",
    "    line2, = ax2.plot(xGraph, yGraph, c='grey')\n",
    "    point2, = ax2.plot(x2, y2, 'o', c='indianred')\n",
    "    \n",
    "    ax3.clear()\n",
    "    ax3.set_title(f'eta={eta3}')\n",
    "    line3, = ax3.plot(xGraph, yGraph, c='grey')\n",
    "    point3, = ax3.plot(x3, y3, 'o', c='indianred')\n",
    "    \n",
    "    fig.suptitle(f'Iteration {i:.0f}/{nIter}', size=14)\n",
    "    \n",
    "    return line1, point1, line2, point2, line3, point3\n",
    "\n",
    "gif = FuncAnimation(fig, animate, frames, interval=200, blit=False, repeat=True)\n",
    "plt.close(fig)\n",
    "\n",
    "path = 'output/adam.gif'\n",
    "gif.save(path, dpi=300, writer=PillowWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b1557-02a1-4868-825c-d1ac32cf9554",
   "metadata": {},
   "source": [
    "## 2.9. Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116bd17-0668-453b-bed1-829919fa1d4c",
   "metadata": {},
   "source": [
    "## 2.10. AdaMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1a70c-75d8-4977-a2ca-2c18df3b80b9",
   "metadata": {},
   "source": [
    "## 2.11. AMSGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c449546-9053-4f45-b91d-9ff5be3706f1",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Ruder] [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html)\n",
    "- [D2L] [Optimization algorithms](http://d2l.ai/chapter_optimization/index.html)\n",
    "- [Distill] [Why Momentum really works](https://distill.pub/2017/momentum/)\n",
    "- [CompPhysics] [Optimization and Gradient Methods](https://compphysics.github.io/MachineLearningMSU/doc/pub/GradientOptim/html/._GradientOptim-bs000.html)\n",
    "- [TowardsDataScience] [Stochastic Gradient Descent with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)\n",
    "- [PaperspaceBlog] [Intro to optimization in deep learning: Momentum, RMSProp and Adam](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
