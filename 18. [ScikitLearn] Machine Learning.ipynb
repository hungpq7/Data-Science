{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Machine Learning is the study of computer algorithms that improve automatically through experience. Machine Learning algorithms build a mathematical model based on sample data in order to make predictions or decisions. In Data Analytics, Machine Learning is referred to as predictive analysis and data mining.\n",
    "\n",
    "*Reference: [Wikipedia - Machine Learning](https://en.wikipedia.org/wiki/Machine_learning)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning branches\n",
    "\n",
    "<img src='image/ml_branches.png' style='height:400px; margin: 0 auto 10px;'>\n",
    "\n",
    "Here is how Machine Learning applied in Data Analytics:\n",
    "- Supervised learning refers to algorthims which predict output of new data. The computer is trained with labeled data, the observations is the question and the labels is the answer. The final goal is to learn a general rules that map input (features) to output (labels). Supervised learning is used in predictive analysis.\n",
    "- Unsupervised learning refers to the algorithms not require any label, leaving it on its own to find structures in the data. Unsupervised learning allows discovering hidden patterns in data, and is widely used in data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing to Statistics\n",
    "The major difference between Machine Learning and Statistics is their purposes. Machine Learning algorithms are designed to make the most accurate predictions possible. Statistical algorithms are designed for inference about the relationships between variables.\n",
    "\n",
    "*Reference: [Towards Data Science - The actual difference between Statistics and Machine Learning](https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Terminology\n",
    "This section focus on concepts that occur in supervised Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Miscellaneous concepts\n",
    "https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Training\n",
    "Training is the process of generating a particular model from an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "A model is the output of a Machine Learning algorithm ran on the data. For example, the Linear Regression algorithm results in a model being a vector of parameters (also known as weights). Machine Learning uses mathematical techniques to estimate those parameters.\n",
    "\n",
    "After the parameters have been calculated, the model is saved as a file and can be used on the data it hasn't seen before to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "Loss function is a function that maps a model to its associated cost. In Machine Learning, loss functions usually evaluate the error, for example the difference between real values and predicted values. Each algorithm tries to minimize its own loss function to estimate the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "In multivariate calculus, the gradient of a multivariable function $f(x,y,z,\\dots)$, denoted $\\nabla{f}$ is a vector that stores all partial derivatives.\n",
    "\n",
    "$$\\nabla{f} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{f}}{\\partial{x}} &\n",
    "\\frac{\\partial{f}}{\\partial{y}} &\n",
    "\\frac{\\partial{f}}{\\partial{z}} &\n",
    "\\cdots &\n",
    "\\end{bmatrix}^T$$\n",
    "\n",
    "At every local minimum or local maximum of the function $f(x,y,z,\\dots)$, the gradient $\\nabla{f}$ is $\\mathbf{0}$. By solving the linear system, those extreme points can be found. Then compare the extrema to find the minimum and maximum. The steps above show how to use gradient to find the minimum of a loss function and the associated values of parameters. However, for very large linear systems, solving them directly seems impossible.\n",
    "\n",
    "Therefore, an iterative method called gradient descent is commonly used as the optimization algorithm in Machine Learning. The algorithm begins at a point (initial value), moves in the opposite direction of the gradient and stops when $\\|\\nabla{f}\\|_2 \\approx 0$.\n",
    "\n",
    "Gradient descent has a parameter, $\\eta$ - the learning rate, represents how fast the algorithm goes down. A high learning rate means less iterations to perform, but we risk bypassing the lowest point. A low learning rate makes sure we can always reach the bottom, but it can take a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Validating\n",
    "Validating is the process of finding optimal hyperparameters. Also called hyperparameters tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "A hyperparameter is a configuration which is external to the model. Hyperparameters cannot be estimated since they need to be set before fitting the algorithm. They are optimized using rules of thumb or trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "Metrics are the formulas that evaluate the performance of Machine Learning models. The metrics are selected depend on the type of algorithm, which can be either regression, classification or clustering. Also notice that multiple metrics can be used on a single model; and the loss function can also be used as a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation\n",
    "The final purpose of a Machine Learning model is to work well on completely new data. To achieve this, we perform validation:\n",
    "- Step 1: Randomly split the data into 3 sets: training set, validation set and test set. The ratio is usually 70:15:15 or 80:10:10.\n",
    "- Step 2: Fit the algorithm on the training data. Then use the output model to predict label for the validation set and evaluate the performance.\n",
    "- Step 3: Repeat step 2 with different values of hyperparameters to find the model with best performance.\n",
    "- Step 4: Apply the model to the test data and calculate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "Considering validation, when the data is not large enough, the training set and the test set may not distribute the same way, which leads to bias. A technique called cross validation my be used to handle the situation.\n",
    "- Step 1: Randomly devide the entire dataset into $k$ equal folds ($k$ is usually from 5 to 10).\n",
    "- Step 2: Run a loop through each of the $k$ folds. In each iteration:\n",
    "    - Take the selected fold as the test set and take the $k-1$ remaining folds to form the training set\n",
    "    - Fit the algorithm on the training set and evaluate it on the test set\n",
    "    - Retain the metric and discard the model\n",
    "- Step 3: Fit the algorithm on the entire data to get the final model. Then calculate the average of the recorded scores as the overall performance metric for the model. The standard deviation of the scores should not be too high.\n",
    "\n",
    "<img src='image/cross_validation.png' style='height:300px; margin: 0 auto 20px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodness of fit\n",
    "Generalization is the ability of a model to give sensible output when given the data is has never seen before. A model generalizes well if it is neither underfit nor overfit.\n",
    "\n",
    "<img src='image/goodness_of_fit.png' style='height:300px; margin: 0 auto 20px;'>\n",
    "\n",
    "Overfitting refers to the situation that the performance of a model is very good on the training set but drops significantly over the test set. This can be explained that the model has learned the noise and random fluctuations from the training data, which have negative impacts on new data.\n",
    "\n",
    "Underfitting can be regconized when the model performs poorly on both the test and the training set. The reason for this phenomenon is that the model did not learn enough patterns from the training data, so obviously there is no way it can work well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation\n",
    "*Reference: [Scikit-Learn - Metrics](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Classification visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'data\\iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [1, 7]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "Confusion matrix is a visualization for the performance of classification algorithms. Each column represents the number of observations in an actual class and each row represents the number of observations in a predicted class.\n",
    "\n",
    "<img src='image/confusion_matrix_binary.png' style='height:300px; margin: 0 auto 20px;'>\n",
    "\n",
    "However, in real-world problems, sometimes a class is more important than the others. From the perspective of that class, \"Orange\" for example, an observation may have the following statuses:\n",
    "- Positive if the predicted class is orange, Negative if the predicted class is not orange\n",
    "- True if the predicted class is correct, False if the predicted class is incorrect\n",
    "\n",
    "The combination of the statuses above returns 4 quatities: TP (True Positive), TN (True Negative), FP (False Positive) and FN (False Negative), which are used to calculate metrics for classification algorithms. These terms are arranged in a single table:\n",
    "\n",
    "<img src='image/confusion_matrix.png' style='height:300px; margin: 0 auto;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve\n",
    "The ROC (Receiver Operating Characteristics) curve is plotted with TPR (True Positive Rate) against FPR (False Positive Rate) at different decision thresholds. It gives an overview about how well the model distinguish between the two classes.\n",
    "\n",
    "$$\\mathrm{TPR}=\\mathrm{Recall}=\\mathrm{Sensitivity} = \n",
    "\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$$\n",
    "\n",
    "$$\\mathrm{FPR}=1-\\mathrm{Specificity} = \n",
    "\\frac{\\mathrm{FP}}{\\mathrm{TN}+\\mathrm{FP}}$$\n",
    "\n",
    "In real world problems, there is always a trade-off between high Sensitivity and high Specificity. By looking at the ROC curve, there are many insights can be read from it:\n",
    "- Comaparing the quality of different models. The more convex the curve is, the more predictive power it has.\n",
    "- Determining the optimal threshold.\n",
    "- Calculating the AUROC (Area Under the ROC), which is a single value summarizes the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lift Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "Accuracy treats all the classes equally, since it calculates the True rate, which is the number of correct predictions over the total of predictions made. Accuracy is the default classification metric used in `sklearn`.\n",
    "\n",
    "$$\\mathrm{Accuracy} = \\frac{\\mathrm{T}}{\\mathrm{T}+\\mathrm{F}}$$\n",
    "\n",
    "Accuracy is a very good measure when the classes are (nearly) balanced, however it should never be used when the majority of the data fall into a single class. For example, there are 5 people have cancer out of 100 observations, and a predictive model classifies all 100 people have no cancer. So even it is very terrible at predicting cancer, such a bad model still has the Accuracy of 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "The Precision of a class shares the same meaning with Accuracy, but it considers only Positive predictions. It is calculated as the proportion of those Positive predictions are actually Positive.\n",
    "\n",
    "$$\\mathrm{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$$\n",
    "\n",
    "Precision should be used when you want to make classification for one class as good as possible. For example, a video recommendation system classifies Positive for revalent videos and Negative for non-revalent ones. The recommended videos should be revalent to the users, so the system should have a high Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "The per-class Recall, also know as Sensitivity, measures the proportion of actual Positive observations are correctly classified.\n",
    "\n",
    "$$\\mathrm{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$$\n",
    "\n",
    "If you don't want to mispredict any real Positive case, then you need Recall. For example, a COVID-19 test kit must detect as many infected people (Positive) as possible, so it should have a high Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-score\n",
    "Machine Learning is full of trade-offs, oftentimes your classifier does not have good Precision and good Recall at the same time. A single metric that summarizes both is needed, and that's where F-score is used.\n",
    "\n",
    "$$\n",
    "F_1 = \\left(\\frac{1}{2}\\cdot\\mathrm{Precision}^{-1}+\\frac{1}{2}\\cdot\\mathrm{Recall}^{-1}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "To compute $F_1$ score, the harmonic mean is used instead of the usual arithmetic mean since this method gives a higher weight to the lower quantity of the two. For example, a classifier has a Precision of 10% and a Recall of 90%, then their harmonic mean is 18% while their arithmetic mean is 50%. So, a high $F_1$ score ensures that the Precision and Recall are both high.\n",
    "\n",
    "However, there is a disadvantage of $F_1$ score is that it treats both Precision and Recall equally. Sometimes it is required to include domain knowledge in the model, specificly more Recall or more Precision. To solve this, we add a weight to Precision, denoted $\\beta$, to control the trade-off between Precision and Recall. $\\beta$ implies how many times is Recall more important than Precision.\n",
    "\n",
    "$$\n",
    "F_{\\beta} = \\left(\\frac{1}{1+\\beta^2}\\cdot\\mathrm{Precision}^{-1} + \\frac{\\beta^2}{1+\\beta^2}\\cdot\\mathrm{Recall}^{-1}\\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class metrics\n",
    "Notice that except Accuracy, all the above metrics are defined for binary classification. In multi-class problems, they can only measure the performance on a class versus the rest. Here is how the overall metric of the model is calculated, let's take Precision as an example:\n",
    "- Micro Precision, by taking the avarage TP and FP for each class, then apply the same per-class formula.\n",
    "\n",
    "$$\\mathrm{MicroPrecision}=\\frac{\\sum{\\mathrm{TP}_i}}{\\sum{\\mathrm{TP}_i}+\\sum{\\mathrm{FN}_i}}$$\n",
    "\n",
    "- Macro Precision, by taking the arithmetic mean of Precision of all classes ($n$ is the number of classes).\n",
    "\n",
    "$$\\mathrm{MacroPrecision}=\\frac{1}{n}\\sum{\\mathrm{Precision}_i}$$\n",
    "\n",
    "- Weighted Precision, by taking the weighted sum of Precision of all classes. The weight $w_i$ is defined as the proportion of size of each class.\n",
    "\n",
    "$$\\mathrm{WeightedPrecision}=\\sum{w_i\\cdot\\mathrm{Precision}_i}$$\n",
    "\n",
    "The `sklearn` library provides the `classification_report` function that calculates both per-class metrics and overall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a custom metric for validation\n",
    "The default metric used in `sklearn` is Accuracy, however you can use another one using the `scoring` parameter. It takes a string as input, where available options can be found in the reference. For a complex metric, the function requires to define it first.\n",
    "\n",
    "*Reference: [Scikit-Learn - Defining model evaluation rules](https://scikit-learn.org/stable/modules/model_evaluation.html?fbclid=IwAR1sIqgakJmfoA51DXOTMcPAxTujWvuJbwfal3yoV8wFI6WWuJHr7tKzKUg#scoring-parameter)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "params_grid = {'n_neighbors': [3, 4, 5]}\n",
    "metric = make_scorer(fbeta_score, beta=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearch(clf, params_grid, scoring='precision')\n",
    "cv = GridSearch(clf, params_grid, scoring=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Regression metrics\n",
    "Let $\\mathbf{y}=(y_1,y_2,\\dots)$ is the vector of true data labels and $\\hat{\\mathbf{y}}=(\\hat{y}_1,\\hat{y}_2,\\dots)$ is the vector of predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient of Determination\n",
    "Coefficient of Determination, denoted $R^2$ $(R^2 \\leq 1)$, evaluates the scatter of data point around the fitted regression function. It is calculated using $\\mbox{SSE}$ (sum of squared errors) and $\\mbox{SST}$ (total sum of squares).\n",
    "\n",
    "$$R^2\n",
    "=1-\\dfrac{\\mbox{SSE}}{\\mbox{SST}}\n",
    "=1-\\dfrac{\\sum(y_i-\\hat{y}_i)^2}{\\sum(y_i-\\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error\n",
    "MAE is simply the average distance between the real and the predicted values. The absolute value is taken to ensure they do not cancel each other out. MAE does not fall into any certain range, it itself cannot tell if the model is good enough, you can only use MAE to compare the performance of different models.\n",
    "\n",
    "$${\\mbox{MAPE}}={\\frac{1}{n}}\\sum|y_i-\\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error\n",
    "MSE is quite similar to MAE, however MSE gives higher penalization to big error. In real world problems, RMSE (Root Mean Squared Error) is used more commonly since it brings MSE back to the same unit with MAE.\n",
    "\n",
    "$$\\mbox{MSE} = \\mbox{RMSE}^2 = {\\frac{1}{n}} \\sum(y_i-\\hat{y}_i)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Clustering metrics\n",
    "*Reference: [Scikit-Learn - Clustering evaluation performance](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)*\n",
    "\n",
    "Notation:\n",
    "- $\\mathbf{p}_i$ (for $i=1,2,\\dots,n$) is a data point\n",
    "- $C_i$ (for $i=1,2,\\dots,k$) is a cluster\n",
    "- $|C_i|$ (for $i=1,2,\\dots,k$) is the size of cluster $C_i$\n",
    "- $\\mathbf{c}_i$ (for $i=1,2,\\dots,k$) is the centroid of the cluster $C_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient\n",
    "- For each data point $\\mathbf{p}_i$, assume it belongs to the cluster $C_i$ and calculate its Silhouette value, $s_i$.\n",
    " - Compute $\\displaystyle{d_{ii}=\\frac{1}{|C_i|-1}\\sum_{\\mathbf{p}\\in C_i}{d(\\mathbf{p}_i,\\mathbf{p})}}$, the avarage distance between $\\mathbf{p}_i$ and all other data points in $C_i$.\n",
    " - Compute $\\displaystyle{d_{ij}=\\min_{j}{\\frac{1}{|C_j|}\\sum_{\\mathbf{p}\\in C_j}{d(\\mathbf{p}_i,\\mathbf{p})}}}$, the average distance between $\\mathbf{p}_i$ and all points in the nearest cluster, $C_j $.\n",
    " - Compute the Silhouette value for $\\mathbf{p}_i$: $\\displaystyle{s_i=\\frac{d_{ij}-d_{ii}}{\\max{\\{d_{ii},d_{ij}\\}}}}$. This formula ensures $-1\\leq s_i\\leq 1$, a $s_i$ close to 1 means that the data is appropriately clustered.\n",
    "- Calculate the Silhouette Coefficient for the entire data set:\n",
    "$$\\mathrm{SC}=\\frac{1}{n}\\sum_{i=1}^{n}{s_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabasz Index\n",
    "Calinski-Harabasz Index is calculated using the following formula:\n",
    "\n",
    "$$\\mathrm{CHI}=\\frac{\\mbox{SS}_B}{\\mbox{SS}_W}\\cdot\\frac{n-k}{k-1}$$\n",
    "\n",
    "where:\n",
    "- $\\displaystyle{\\mbox{SS}_B=\\sum_{i=1}^n n\\parallel\\mathbf{c}_i-\\bar{\\mathbf{p}}\\parallel_2^2}$ is the overall between-cluster variance\n",
    "\n",
    "- $\\displaystyle{\\mbox{SS}_W=\\sum_{i=1}^{k}\\sum_{\\mathbf{p}\\in C_i}\\parallel\\mathbf{p}-\\mathbf{c}_i\\parallel_2^2}$ is the overall within-cluster variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin Index\n",
    "The calculation of Davies-Bouldin Index follows these steps:\n",
    "\n",
    "- Step 1: For each cluster $C_i$, calculate the goodness score $D_i$.\n",
    " - Compute the measure of scatter $s_i$ for cluster $C_i$ by taking the average Euclidean distance from each point of $C_i$ to the centroid $\\mathbf{c}_i$. A small $s_i$ means data points in $C_i$ are close to the centroid.\n",
    " - For every cluster $C_j$ other than $C_i$, compute the Euclidean distance $d_{ij}$ between the centroids, $\\mathbf{c}_i$ and $\\mathbf{c}_j$. A great $d_{ij}$ means the two clusters are well separated.\n",
    " - Construct $\\displaystyle{R_{ij}=\\frac{s_i+s_j}{d_{ij}}}$ which measures the goodness of the clustering model. $R_{ij}$ should be as small as possible.\n",
    " - Select $\\displaystyle{D_i=\\max_{j}{R_{ij}}}$ being the worst case of $R_{ij}$.\n",
    "- Step 2: Calculate the Davies-Bouldin Index:\n",
    "$$\\mathrm{DBI}=\\frac{1}{k}\\sum_{i=1}^{k}{D_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "label = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5034774406932958"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(X, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7513707094756764"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davies_bouldin_score(X, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487.33087637489984"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calinski_harabasz_score(X, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
