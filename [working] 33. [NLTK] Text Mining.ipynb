{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is unstructured data and present in all phenomena that occur in reality. The analyzed text helps us to discover and solve many problems in the field of data science.\n",
    "Natural language processing (NLP) is an area of computer science and artificial intelligence (AI) concerned with the interaction between computers and humans in natural language. The goal of NLP is to help computers understand language as well as we do.\n",
    "\n",
    "**Applications of NLP**\n",
    "\n",
    "Information extraction|Text generation|Text classification|Text clustering\n",
    "--:|:--:|:--:|:--\n",
    "Name entity recognition|Writing suggestion|Spam filtering|Topic clustering\n",
    "Job information extraction|Summarization|Document classification\n",
    "Sentiment extraction|Chatbot|Social listening\n",
    "Keyword extraction|News generation|Recommendation\n",
    "\n",
    "*NLTK (Natural Language Toolkit)* is a leading platform for building Python programs to work with human language data. It contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lexical analysis\n",
    "By wikipedia, lexical analysis is the process of converting a sequence of characters into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer or tokenizer. A lexer is the first step of NLP project that its output is the input of parsing process - helping the parsing more easier.\n",
    "\n",
    "<img src='image/lexical.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [^aàảãáạăằẳẵắặâầẩẫấậbcdđeèẻẽéẹêềểễếệfghiìỉĩíịjklmnoòỏõóọôồổỗốộơờởỡớợpqrstuùủũúụưừửữứựvwxyỳỷỹýỵz\\s]\n",
    "# (^|\\s+)(\\S(\\s+|$))+\n",
    "# http\\S+\n",
    "# (?<=phần\\s|tập\\s|t|t.)\\d+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Tokenization is important process because it provide the input for all SOTA model in NLP at token level.\n",
    "For example, consider the sentence: “Never give up”. The most common way of forming tokens is based on space. Assuming space as a delimiter, the tokenization of the sentence results in 3 tokens – *Never-give-up*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences tokenization\n",
    "\n",
    "Sentences tokenize using pre-trained model from `PunktSentenceTokenizer` in NLTK to divide a text into a list of sentences based on recognizing words which start sentences and finding sentence boundaries.\n",
    "\n",
    "*Reference*: [Punkt documentation](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are different methods and libraries available to perform tokenization!',\n",
       " 'NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"There are different methods and libraries available to perform tokenization!\n",
    "NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.',\n",
       " 'And sometimes sentences can start with non-capitalized words.',\n",
       " 'i is a good variable name.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  \n",
    "And sometimes sentences can start with non-capitalized words.  i is a good variable name.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization\n",
    "Word tokenization is the most commonly used tokenization algorithm. It splits a piece of text into individual words based on a certain delimiter (whitespace, puntuation). Depending upon delimiters, different word-level tokens are formed.\n",
    "\n",
    "In NLTK, there are two function `word_tokenize`- is based on a `TreebankWordTokenizer` and `wordpunct_tokenize` is based on a simple regexp tokenization. `wordpunct_tokenize` will split all special symbols and treat them as separate units. `word_tokenize` on the other hand keeps things like single quote together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', ',', 'spacy', ',', 'tokenizer', 'are', 'popular', 'library', 'in', 'text', 'mining']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"NLTK, spacy,tokenizer are popular library in text mining\"\"\"\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sentence', \"'\", 'I', 'won', \"'\", 'can', 'be', 'tokenized', 'into', 'two', 'word-tokens', \"'\", 'I', \"'\", 'and', \"'won\", \"'\", '.', 'There', \"'s\", 'a', 'different', 'between', 'word', 'and', 'wordpunct', 'function', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The sentence 'I won' can be tokenized into two word-tokens 'I' and 'won'. \n",
    "There's a different between word and wordpunct function!\"\"\"\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sentence', \"'\", 'I', 'won', \"'\", 'can', 'be', 'tokenized', 'into', 'two', 'word', '-', 'tokens', \"'\", 'I', \"'\", 'and', \"'\", 'won', \"'.\", 'There', \"'\", 's', 'a', 'different', 'between', 'word', 'and', 'wordpunct', 'function', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The sentence 'I won' can be tokenized into two word-tokens 'I' and 'won'. \n",
    "There's a different between word and wordpunct function!\"\"\"\n",
    "\n",
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Stemming\n",
    "Stemming and Lemmatization are Text Normalization (or Word Normalization) techniques in the field of NLP that are used to prepare text, words, and documents for further processing. Stemming is different to Lemmatization in the approach it uses to produce root forms of words.\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem. Stemming a word or sentence may result in words that are not actual words (word has no meaning in dictionary). Stems are created by removing the suffixes or prefixes used with a word.\n",
    "\n",
    "In NLTK, there are two popular stemming algorithms for English: PorterStemmer and LancasterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer\n",
    "\n",
    "This stemming algorithm is an older one. It uses suffix stripping to produce stems and does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. That why it sometime generate not a english word. PorterStemmer is known for its simplicity and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trouble --> troubl\n",
      "troubling --> troubl\n",
      "working --> work\n",
      "worked --> work\n",
      "friendship --> friendship\n",
      "friendly --> friendli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "words = ['trouble','troubling','working','worked','friendship','friendly']\n",
    "for word in words:\n",
    "    print(word+' --> '+porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stemming is the process of producing morphological variants of a root'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming is not working with a sentence\n",
    "sentence=\"Stemming is the process of producing morphological variants of a root\"\n",
    "porter.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem is the process of produc morpholog variant of a root word \n"
     ]
    }
   ],
   "source": [
    "sentence=\"Stemming is the process of producing morphological variants of a root word\"\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "print(stemSentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LancasterStemmer\n",
    "\n",
    "LancasterStemmer which is the most aggressive stemming algorithm of the bunch is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. One complaint around this stemming algorithm though is that it sometimes is over-stemming and can really transform words into strange stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> run\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easy\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trouble --> troubl\n",
      "troubling --> troubl\n",
      "working --> work\n",
      "worked --> work\n",
      "friendship --> friend\n",
      "friendly --> friend\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "words = ['trouble','troubling','working','worked','friendship','friendly']\n",
    "for word in words:\n",
    "    print(word+' --> '+lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem is the process of produc morpholog vary of a root word \n"
     ]
    }
   ],
   "source": [
    "sentence=\"Stemming is the process of producing morphological variants of a root word\"\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(lancaster.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "print(stemSentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming but it brings context to the words. It links words with similar meaning to one word while stemming is just removing the surfix/prefix of word without concern to meaning of the words.\n",
    "\n",
    "Lemmatization work best if it was provided second argument to `lemmatize()` with correct part of speech (POS) tagging.\n",
    "\n",
    "*Part of speech constant*\n",
    "\n",
    "Type|Sign\n",
    "--:|:--:|\n",
    "ADJ|`a`\n",
    "ADV|`r`\n",
    "Noun|`n`\n",
    "Verb|`v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer  \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easily --> easily\n",
      "fairly --> fairly\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+lemmatizer.lemmatize(word, pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> run\n",
      "runs --> run\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_verb = ['run','runner','running','ran','runs']\n",
    "for word in words_verb:\n",
    "    print(word+' --> '+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feet --> foot\n",
      "teeth --> teeth\n",
      "bats --> bat\n",
      "mice --> mouse\n",
      "cookies --> cooky\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_noun = ['feet','teeth','bats','mice','cookies']\n",
    "for word in words_noun:\n",
    "    print(word+' --> '+lemmatizer.lemmatize(word, pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Removing stop word \n",
    "\n",
    "Removing stop word is a step in pre-processing text data. A stop word is a commonly used word (such as *the, a, an, in*) that a search engine has been programmed to ignore because they usually bring very litle userful information. Here are a few key benefits of removing stopwords:\n",
    "\n",
    "- On removing stopwords, dataset size decreases and the time to train the model decreases and the accuracy of classification will be increase \n",
    "- Even search engines like Google remove stopwords for fast and relevant retrieval of data from the database.\n",
    "- It suitable for text classification and text generation\n",
    "\n",
    "But removing stop word is not suitable for text summarization and machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentence = \"\"\"The process of converting data to something a computer can understand \n",
    "is referred to as pre-processing\"\"\" \n",
    "word_tokens = word_tokenize(sentence)\n",
    " \n",
    "remove_sw = [i for i in word_tokens if i.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process',\n",
       " 'converting',\n",
       " 'data',\n",
       " 'something',\n",
       " 'computer',\n",
       " 'understand',\n",
       " 'referred',\n",
       " 'pre-processing']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorizing\n",
    "\n",
    "Machine Learning algorithms requires numeric input and do not process the string or raw text. To convert the text data into numerical data, there is a method which are known as vectorization, or Word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bag of word (BoW)\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "1. A vocabulary of words.\n",
    "2. A measure of the presence of words.\n",
    "\n",
    "Any information about the order or structure of words in the document is discarded. BOW is only concerned with whether  words occur in the document. In the simple way, BOW collect all unique token from a document and each sentence will be vectorize by counting each word in BOW.\n",
    "\n",
    "Bow is simple to implement but it don't concern with ordered of word and if the document is very large then the length vector will increase that can lead to sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster =LancasterStemmer()\n",
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(sentence):\n",
    "    alltoken = []\n",
    "    sen = sentence.lower()\n",
    "    tokens = re_tokenizer.tokenize(sen)\n",
    "    for token in tokens:\n",
    "        new_token = lancaster.stem(token)\n",
    "        alltoken.append(token)\n",
    "    return alltoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=[]\n",
    "for s in corpus:\n",
    "    w = tokenize(s)\n",
    "    bow+=w\n",
    "bow=set(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens):\n",
    "    vector=[]\n",
    "    for w in bow:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0]\n",
      "[0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1]\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(corpus)):\n",
    "    vector = vectorize(tokenize(corpus[i]))\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another way to create BoW with Sklearn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "sentence_1=corpus[0]\n",
    "sentence_2=corpus[1]\n",
    "sentence_3=corpus[2]\n",
    " \n",
    "CountVec = CountVectorizer(stop_words='english')\n",
    "#transform\n",
    "count_data = CountVec.fit_transform([sentence_1,sentence_2,sentence_3])\n",
    " \n",
    "count_data.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Term frequency-inverse document frequency (TF-IDF)\n",
    "\n",
    "If a word occurs many times in a document but also along with many other documents in a dataset, maybe it is because this word is just a frequent word, not because it is relevant or meaningful. One approach of TF-IDF is to rescale the frequency of words by how often they appear in all documents so the scores for frequent words like *the* which are also frequent across all documents are penalized. It divided to two process:\n",
    "\n",
    "*1. Term frequency process:* \\\n",
    "This measures the frequency of a word in a document - but to prevent the large amount of no meaningful word, TF perform a normalization on the frequency value. It divide the frequency with the total number of words in the document.\n",
    "$$TF(t,d) = \\frac{n_{t,d}}{N_d}$$\n",
    "\n",
    "$n_{t,d}$: number of word $t$ in document $d$\\\n",
    "$N_d$: total number of words in document $d$\n",
    "\n",
    "*2. Inverse document frequency:* \\\n",
    "DF - document frequency measures the importance of document in whole set of corpus. It's the number of documents in which the word is present. DF consider one occurrence if the term consists in the document at least once, and do not need to know the number of times the term is present.\n",
    "IDF is the inverse of the DF which measures the informativeness of term t. When calculate IDF, it will be very low for the most occurring words such as stop words. Using log to calculate IDF to prevent explode in case the corpus is very large.\n",
    "$$IDF(t) = log(\\frac{N}{df_t})$$\n",
    "\n",
    "$df_t$ : number of document containing the word $t$\\\n",
    "$N$: total number of document\n",
    "$$\\mbox{TF-IDF(t,d) = TF(t,d) * IDF(t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>bat</th>\n",
       "      <th>bats</th>\n",
       "      <th>can</th>\n",
       "      <th>door</th>\n",
       "      <th>echolocation</th>\n",
       "      <th>elephant</th>\n",
       "      <th>of</th>\n",
       "      <th>opened</th>\n",
       "      <th>potatoes</th>\n",
       "      <th>see</th>\n",
       "      <th>she</th>\n",
       "      <th>sight</th>\n",
       "      <th>sneeze</th>\n",
       "      <th>sneezed</th>\n",
       "      <th>studio</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>via</th>\n",
       "      <th>wondering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.378676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378676</td>\n",
       "      <td>0.378676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230069</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302514</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367724</td>\n",
       "      <td>0.434367</td>\n",
       "      <td>0.367724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         at       bat      bats       can      door  echolocation  elephant  \\\n",
       "0  0.378676  0.000000  0.000000  0.000000  0.000000      0.000000  0.378676   \n",
       "1  0.000000  0.302514  0.302514  0.302514  0.000000      0.302514  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.367724      0.000000  0.000000   \n",
       "\n",
       "         of    opened  potatoes       see       she     sight    sneeze  \\\n",
       "0  0.378676  0.000000  0.378676  0.000000  0.000000  0.287993  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.605027  0.000000  0.230069  0.302514   \n",
       "2  0.000000  0.367724  0.000000  0.000000  0.367724  0.000000  0.000000   \n",
       "\n",
       "    sneezed    studio       the        to       via  wondering  \n",
       "0  0.378676  0.000000  0.447305  0.000000  0.000000   0.000000  \n",
       "1  0.000000  0.000000  0.178669  0.000000  0.302514   0.000000  \n",
       "2  0.000000  0.367724  0.434367  0.367724  0.000000   0.367724  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([corpus[0], corpus[1],corpus[2]])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Syntactic processing\n",
    "\n",
    "Syntactic analysis or parsing is defined as the process of analyzing the strings of symbols in natural language conforming to the rules of formal grammar. The purpose of this process is to draw exact meaning, or perform dictionary meaning from the text. Syntax analysis checks the text for meaningfulness comparing to the rules of formal grammar.\n",
    "\n",
    "Example:\n",
    "1. Delhi is the capital of India.\n",
    "2. Is Delhi the of India capital.\n",
    "\n",
    "Two sentences have the same word but only sentence 1 was meaningful and syntactically correct. The purpose of sytactic processing is recover the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Part of speech (POS) tagging \n",
    "\n",
    "Part of speech is the process  which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context. To analyze the relationship and understanding meaning of text, pos tagging is very important process. POS tag are useful for building parse trees, which are used in building NERs and extracting relations between words. It is also use for building lemmatizers in 1.3 \n",
    "\n",
    "Some pos tagging techniques:\n",
    "- Lexical base method: Assigns the POS tag the most frequently occurring with a word in the training corpus\n",
    "- Rule based method: Assigns POS tags based on rules in dictionary.\n",
    "- Probabilistic method: This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.\n",
    "- Deep learning method: Recurrent Neural Networks can also be used for POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Random Fields (CRFs)\n",
    "\n",
    "Conditional Random Fields are a discriminative model, used for predicting sequences. They use contextual information from previous labels, thus increasing the amount of information the model has to make a good prediction.\n",
    "Discriminative classifier - they model the decision boundary between the different classes (just like logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Parsing\n",
    "One of the most important parts of syntactic processing is parsing. It means to break down a given sentence into its *grammatical components*. \n",
    "\n",
    "NLTK doesn't support pre-trained English grammar model, we have to manually specify grammar before parsing a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = 'I really like your shirt'\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "relation=[]\n",
    "head=[]\n",
    "children=[]\n",
    "for token in doc:\n",
    "    tokens.append(token.text)\n",
    "    relation.append(token.dep_)\n",
    "    head.append(token.head.text)\n",
    "    children.append([i for i in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'token':token,\n",
    "                       'relation':relation,\n",
    "                       'head':head,\n",
    "                       'children':children})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>relation</th>\n",
       "      <th>head</th>\n",
       "      <th>children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shirt</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>like</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shirt</td>\n",
       "      <td>advmod</td>\n",
       "      <td>like</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shirt</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>like</td>\n",
       "      <td>[I, really, shirt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shirt</td>\n",
       "      <td>poss</td>\n",
       "      <td>shirt</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shirt</td>\n",
       "      <td>dobj</td>\n",
       "      <td>like</td>\n",
       "      <td>[your]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token relation   head            children\n",
       "0  shirt    nsubj   like                  []\n",
       "1  shirt   advmod   like                  []\n",
       "2  shirt     ROOT   like  [I, really, shirt]\n",
       "3  shirt     poss  shirt                  []\n",
       "4  shirt     dobj   like              [your]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"800eb67eafa148bdb9d31cc8964a8b98-0\" class=\"displacy\" width=\"650\" height=\"257.0\" direction=\"ltr\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">really</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">like</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">your</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">shirt</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-800eb67eafa148bdb9d31cc8964a8b98-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,2.0 290.0,2.0 290.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-800eb67eafa148bdb9d31cc8964a8b98-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-800eb67eafa148bdb9d31cc8964a8b98-0-1\" stroke-width=\"2px\" d=\"M190,122.0 C190,62.0 285.0,62.0 285.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-800eb67eafa148bdb9d31cc8964a8b98-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,124.0 L182,112.0 198,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-800eb67eafa148bdb9d31cc8964a8b98-0-2\" stroke-width=\"2px\" d=\"M430,122.0 C430,62.0 525.0,62.0 525.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-800eb67eafa148bdb9d31cc8964a8b98-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,124.0 L422,112.0 438,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-800eb67eafa148bdb9d31cc8964a8b98-0-3\" stroke-width=\"2px\" d=\"M310,122.0 C310,2.0 530.0,2.0 530.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-800eb67eafa148bdb9d31cc8964a8b98-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M530.0,124.0 L538.0,112.0 522.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
