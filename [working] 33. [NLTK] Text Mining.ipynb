{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is unstructured data and present in all phenomena that occur in reality. The analyzed text helps us to discover and solve many problems in the field of data science.\n",
    "Natural language processing (NLP) is an area of computer science and artificial intelligence (AI) concerned with the interaction between computers and humans in natural language. The goal of NLP is to help computers understand language as well as we do.\n",
    "\n",
    "**Applications of NLP**\n",
    "\n",
    "Information extraction|Text generation|Text classification|Text clustering\n",
    "--:|:--:|:--:|:--\n",
    "Name entity recognition|Writing suggestion|Spam filtering|Topic clustering\n",
    "Job information extraction|Summarization|Document classification\n",
    "Sentiment extraction|Chatbot|Social listening\n",
    "Keyword extraction|News generation|Recommendation\n",
    "\n",
    "*NLTK (Natural Language Toolkit)* is a leading platform for building Python programs to work with human language data. It contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lexical analysis\n",
    "By wikipedia, lexical analysis is the process of converting a sequence of characters into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer or tokenizer. A lexer is the first step of NLP project that its output is the input of parsing process - helping the parsing more easier.\n",
    "\n",
    "<img src='image/lexical.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Tokenization is important process because it provide the input for all SOTA model in NLP at token level.\n",
    "For example, consider the sentence: “Never give up”. The most common way of forming tokens is based on space. Assuming space as a delimiter, the tokenization of the sentence results in 3 tokens – *Never-give-up*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences tokenization\n",
    "\n",
    "Sentences tokenize using pre-trained model from `PunktSentenceTokenizer` in NLTK to divide a text into a list of sentences based on recognizing words which start sentences and finding sentence boundaries.\n",
    "\n",
    "*Reference*: [Punkt documentation](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are different methods and libraries available to perform tokenization!',\n",
       " 'NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"There are different methods and libraries available to perform tokenization!\n",
    "NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.',\n",
       " 'And sometimes sentences can start with non-capitalized words.',\n",
       " 'i is a good variable name.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  \n",
    "And sometimes sentences can start with non-capitalized words.  i is a good variable name.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization\n",
    "Word tokenization is the most commonly used tokenization algorithm. It splits a piece of text into individual words based on a certain delimiter (whitespace, puntuation). Depending upon delimiters, different word-level tokens are formed.\n",
    "\n",
    "In NLTK, there are two function `word_tokenize`- is based on a `TreebankWordTokenizer` and `wordpunct_tokenize` is based on a simple regexp tokenization. `wordpunct_tokenize` will split all special symbols and treat them as separate units. `word_tokenize` on the other hand keeps things like single quote together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', ',', 'spacy', ',', 'tokenizer', 'are', 'popular', 'library', 'in', 'text', 'mining']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"NLTK, spacy,tokenizer are popular library in text mining\"\"\"\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sentence', \"'\", 'I', 'won', \"'\", 'can', 'be', 'tokenized', 'into', 'two', 'word-tokens', \"'\", 'I', \"'\", 'and', \"'won\", \"'\", '.', 'There', \"'s\", 'a', 'different', 'between', 'word', 'and', 'wordpunct', 'function', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The sentence 'I won' can be tokenized into two word-tokens 'I' and 'won'. \n",
    "There's a different between word and wordpunct function!\"\"\"\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sentence', \"'\", 'I', 'won', \"'\", 'can', 'be', 'tokenized', 'into', 'two', 'word', '-', 'tokens', \"'\", 'I', \"'\", 'and', \"'\", 'won', \"'.\", 'There', \"'\", 's', 'a', 'different', 'between', 'word', 'and', 'wordpunct', 'function', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The sentence 'I won' can be tokenized into two word-tokens 'I' and 'won'. \n",
    "There's a different between word and wordpunct function!\"\"\"\n",
    "\n",
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Stemming\n",
    "Stemming and Lemmatization are Text Normalization (or Word Normalization) techniques in the field of NLP that are used to prepare text, words, and documents for further processing. Stemming is different to Lemmatization in the approach it uses to produce root forms of words.\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem. Stemming a word or sentence may result in words that are not actual words (word has no meaning in dictionary). Stems are created by removing the suffixes or prefixes used with a word.\n",
    "\n",
    "In NLTK, there are two popular stemming algorithms for English: PorterStemmer and LancasterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer\n",
    "\n",
    "This stemming algorithm is an older one. It uses suffix stripping to produce stems and does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. That why it sometime generate not a english word. PorterStemmer is known for its simplicity and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trouble --> troubl\n",
      "troubling --> troubl\n",
      "working --> work\n",
      "worked --> work\n",
      "friendship --> friendship\n",
      "friendly --> friendli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "words = ['trouble','troubling','working','worked','friendship','friendly']\n",
    "for word in words:\n",
    "    print(word+' --> '+porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stemming is the process of producing morphological variants of a root'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming is not working with a sentence\n",
    "sentence=\"Stemming is the process of producing morphological variants of a root\"\n",
    "porter.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem is the process of produc morpholog variant of a root word \n"
     ]
    }
   ],
   "source": [
    "sentence=\"Stemming is the process of producing morphological variants of a root word\"\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "print(stemSentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LancasterStemmer\n",
    "\n",
    "LancasterStemmer which is the most aggressive stemming algorithm of the bunch is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. One complaint around this stemming algorithm though is that it sometimes is over-stemming and can really transform words into strange stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> run\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easy\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trouble --> troubl\n",
      "troubling --> troubl\n",
      "working --> work\n",
      "worked --> work\n",
      "friendship --> friend\n",
      "friendly --> friend\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "words = ['trouble','troubling','working','worked','friendship','friendly']\n",
    "for word in words:\n",
    "    print(word+' --> '+lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem is the process of produc morpholog vary of a root word \n"
     ]
    }
   ],
   "source": [
    "sentence=\"Stemming is the process of producing morphological variants of a root word\"\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(lancaster.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "print(stemSentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming but it brings context to the words. It links words with similar meaning to one word while stemming is just removing the surfix/prefix of word without concern to meaning of the words.\n",
    "\n",
    "Lemmatization work best if it was provided second argument to `lemmatize()` with correct part of speech (POS) tagging.\n",
    "\n",
    "*Part of speech constant*\n",
    "\n",
    "Type|Sign\n",
    "--:|:--:|\n",
    "ADJ|`a`\n",
    "ADV|`r`\n",
    "Noun|`n`\n",
    "Verb|`v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer  \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easily --> easily\n",
      "fairly --> fairly\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+lemmatizer.lemmatize(word, pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> run\n",
      "runs --> run\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_verb = ['run','runner','running','ran','runs']\n",
    "for word in words_verb:\n",
    "    print(word+' --> '+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feet --> foot\n",
      "teeth --> teeth\n",
      "bats --> bat\n",
      "mice --> mouse\n",
      "cookies --> cooky\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_noun = ['feet','teeth','bats','mice','cookies']\n",
    "for word in words_noun:\n",
    "    print(word+' --> '+lemmatizer.lemmatize(word, pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Removing stop word \n",
    "\n",
    "Removing stop word is a step in pre-processing text data. A stop word is a commonly used word (such as *the, a, an, in*) that a search engine has been programmed to ignore because they usually bring very litle userful information. Here are a few key benefits of removing stopwords:\n",
    "\n",
    "- On removing stopwords, dataset size decreases and the time to train the model decreases and the accuracy of classification will be increase \n",
    "- Even search engines like Google remove stopwords for fast and relevant retrieval of data from the database.\n",
    "- It suitable for text classification and text generation\n",
    "\n",
    "But removing stop word is not suitable for text summarization and machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentence = \"\"\"The process of converting data to something a computer can understand \n",
    "is referred to as pre-processing\"\"\" \n",
    "word_tokens = word_tokenize(sentence)\n",
    " \n",
    "remove_sw = [i for i in word_tokens if i.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process',\n",
       " 'converting',\n",
       " 'data',\n",
       " 'something',\n",
       " 'computer',\n",
       " 'understand',\n",
       " 'referred',\n",
       " 'pre-processing']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Syntactic processing\n",
    "\n",
    "Syntactic analysis or parsing is defined as the process of analyzing the strings of symbols in natural language conforming to the rules of formal grammar. The purpose of this process is to draw exact meaning, or perform dictionary meaning from the text. Syntax analysis checks the text for meaningfulness comparing to the rules of formal grammar.\n",
    "\n",
    "Example:\n",
    "1. Delhi is the capital of India.\n",
    "2. Is Delhi the of India capital.\n",
    "\n",
    "Two sentences have the same word but only sentence 1 was meaningful and syntactically correct. The purpose of sytactic processing is recover the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Parsing\n",
    "One of the most important parts of syntactic processing is parsing. It means to break down a given sentence into its *grammatical components*. \n",
    "\n",
    "NLTK doesn't support pre-trained English grammar model, we have to manually specify grammar before parsing a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Part of speech (POS) tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Semantic processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
