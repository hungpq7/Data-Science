{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is unstructured data and present in all phenomena that occur in reality. The analyzed text helps us to discover and solve many problems in the field of data science.\n",
    "Natural language processing (NLP) is an area of computer science and artificial intelligence (AI) concerned with the interaction between computers and humans in natural language. The goal of NLP is to help computers understand language as well as we do.\n",
    "\n",
    "**Applications of NLP**\n",
    "\n",
    "Information extraction|Text generation|Text classification|Text clustering\n",
    "--:|:--:|:--:|:--\n",
    "Name entity recognition|Writing suggestion|Spam filtering|Topic clustering\n",
    "Job information extraction|Summarization|Document classification\n",
    "Sentiment extraction|Chatbot|Social listening\n",
    "Keyword extraction|News generation|Recommendation\n",
    "\n",
    "*NLTK (Natural Language Toolkit)* is a leading platform for building Python programs to work with human language data. It contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenizing\n",
    "\n",
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Tokenization is important process because it provide the input for all SOTA model in NLP at token level.\n",
    "For example, consider the sentence: “Never give up”. The most common way of forming tokens is based on space. Assuming space as a delimiter, the tokenization of the sentence results in 3 tokens – *Never-give-up*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences tokenization\n",
    "\n",
    "Sentences tokenize using pre-trained model from `PunktSentenceTokenizer` in NLTK to divide a text into a list of sentences based on recognizing words which start sentences and finding sentence boundaries.\n",
    "\n",
    "*Reference*: [Punkt documentation](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are different methods and libraries available to perform tokenization!',\n",
       " 'NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"There are different methods and libraries available to perform tokenization!\n",
    "NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.',\n",
       " 'And sometimes sentences can start with non-capitalized words.',\n",
       " 'i is a good variable name.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  \n",
    "And sometimes sentences can start with non-capitalized words.  i is a good variable name.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization\n",
    "Word tokenization is the most commonly used tokenization algorithm. It splits a piece of text into individual words based on a certain delimiter. Depending upon delimiters, different word-level tokens are formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Word segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Syntactic processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Morphonology analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Part of speech (POS) tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Semantic processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
