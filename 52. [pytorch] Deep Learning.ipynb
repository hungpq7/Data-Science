{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[PyTorch] is one of the two most popular Deep Learning frameworks in Python, besides TensorFlow. Here is some key points when comparing the two:\n",
    "- In terms of low or high level, PyTorch falls somewhere in between TensorFlow and Keras. No fit-and-predict interface, must be done by hand.\n",
    "- PyTorch is prefered by research community with more customizations, as we normally see newly published architectures written in PyTorch.\n",
    "- TensorFlow/Keras is better for production due to high-level interface and large deployment ecosystem.\n",
    "\n",
    "[PyTorch]: https://github.com/pytorch/pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import janitor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "In PyTorch, we work most of the time with\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/tensors.html>Tensor</a></code>\n",
    "whose operations are very much like NumPy's array. Being very natural to PyTorch, tensor operations are provided directly in the [mother package]. One thing to notice is that PyTorch requires tensors to be of the same data type so mathematical computation can be performed on them. When error occurs, simply call the <code style='font-size:13px'>double()</code> method to convert the tensor to float type.\n",
    "\n",
    "[mother package]: https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([\n",
    "    [1., 2., 3.],\n",
    "    [4., 5., 6.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1039, 0.6032, 0.2776],\n",
       "        [0.3127, 0.8976, 0.3040]], dtype=torch.float64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand_like(a).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "PyTorch provides automatic differentiation via the sub-module\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/autograd.html>autograd</a></code>,\n",
    "with functions implemented as\n",
    "<code style='font-size:13px'>Tensor</code>\n",
    "methods. Being a mathematical module, it distinguishes two types of tensor, *constant* and *variable*, indicated via the flag <code style='font-size:13px'>requires_grad</code>. All tensors are constants by default, and become variables when this flag is enabled. Autograd is designed to work on a computational graph, where:\n",
    "- The *foward* pass requires inputs to be tensors and output to be a scalar. This pass is done using normal tensor operations.\n",
    "- During *backward* pass (by calling the <code style='font-size:13px'>backward()</code> method on the output), PyTorch will compute and accumulate partial derivatives for leaf nodes. This information can be accessed via the <code style='font-size:13px'>grad</code> attribute of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    y = (x1 - 0.3)**2 + (x2 - 0.7)**2 + 1\n",
    "    return y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T16:05:02.503311Z",
     "iopub.status.busy": "2022-12-28T16:05:02.502943Z",
     "iopub.status.idle": "2022-12-28T16:05:02.507583Z",
     "shell.execute_reply": "2022-12-28T16:05:02.506751Z",
     "shell.execute_reply.started": "2022-12-28T16:05:02.503283Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = torch.rand(8, requires_grad=True)\n",
    "x2 = torch.rand(8, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T16:05:03.349221Z",
     "iopub.status.busy": "2022-12-28T16:05:03.348794Z",
     "iopub.status.idle": "2022-12-28T16:05:03.368533Z",
     "shell.execute_reply": "2022-12-28T16:05:03.367377Z",
     "shell.execute_reply.started": "2022-12-28T16:05:03.349190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0980,  0.0163,  0.0375,  0.0945, -0.0603, -0.0421,  0.0848,  0.0035])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = f(x1, x2)\n",
    "y.backward()\n",
    "x1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "The sub-module <code style='font-size:13px'><a href=https://pytorch.org/docs/stable/optim.html>optim</a></code>\n",
    "implements various optimization algorithms from the basic SGD to the standard Adam. We are going to demonstrate\n",
    "<code style='font-size:13px'>autograd</code> and <code style='font-size:13px'>optim</code>\n",
    "combining together using a simple problem, finding $\\min\\bar{\\mathbf{y}}$ where $\\mathbf{y}=(\\mathbf{x}_1-0.3)^2+(\\mathbf{x}_2-0.7)^2+1$. Recall that all gradient descent algorithms share three components:\n",
    "- (1) configurations such as learning rate $\\eta$, momentum $\\gamma$ and weight decay $\\rho$\n",
    "- (2) the variable $x_t$ and its gradient $g_t$ at each iteration $t$\n",
    "- (3) intermediate variables such as exponential moving averages $m$ and $v$\n",
    "\n",
    "PyTorch implements its optimization algorithms in a low-level interface. During initialization, we set component (1) and register component (2) to our optimizer. Then, when we call the\n",
    "<code style='font-size:13px'>step()</code>\n",
    "method, PyTorch will compute everything needed in component (3) and perform update in-place to $x$.\n",
    "\n",
    "The registration step surprisingly makes sense, as we realize that gradient descent actually only cares about $x$ and never touches $y$. The optimization process is very clearly here: Autograd handles computational graphs to updates $g_t$, then Optim uses $g_t$ to perform a gradient descent step and passes the updated $x_t$ back to Autograd. We can see how smartly PyTorch is designed, each sub-module is account for a specific task, they alternately do their jobs and pass the output to the other. Learning optimization in PyTorch makes understanding about gradient descent much deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    y = (x1 - 0.3)**2 + (x2 - 0.7)**2 + 1\n",
    "    return y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = torch.rand(8, requires_grad=True)\n",
    "x2 = torch.rand(8, requires_grad=True)\n",
    "params = (x1, x2)\n",
    "optimizer = torch.optim.SGD(params, lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nIter = 50\n",
    "for _ in range(nIter):\n",
    "    optimizer.zero_grad()\n",
    "    y = f(x1, x2)\n",
    "    y.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0001, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2962, 0.3104, 0.3003, 0.2940, 0.2979, 0.3084, 0.2987, 0.3042],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6955, 0.6917, 0.6884, 0.6930, 0.7044, 0.7018, 0.6859, 0.6970],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0275e-03,  2.8155e-03,  8.2090e-05, -1.6275e-03, -5.5965e-04,\n",
       "         2.2668e-03, -3.4240e-04,  1.1293e-03])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data loader\n",
    "The sub-module\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/data.html>utils.data</a></code>\n",
    "helps Data Scientists work with different types of data. First, we create a\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/data.html>Dataset</a></code>\n",
    "object to read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 13])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constants\n",
    "df = pd.read_csv('data/boston.csv')\n",
    "x = torch.tensor(df.drop(columns='price').values, dtype=torch.float32)\n",
    "yTrue = torch.tensor(df.price, dtype=torch.float32).reshape(-1, 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularData(Dataset):\n",
    "    def __init__(self, df, labelName):\n",
    "        self.features = df.drop(columns=labelName)\n",
    "        self.label = df[labelName]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   crime_rate  land_rate  indus  chas    nox   room   age  distance  radial  \\\n",
       " 0     0.00632       18.0   2.31     0  0.538  6.575  65.2      4.09       1   \n",
       " \n",
       "    tax  ptratio  black  lstat  \n",
       " 0  296     15.3  396.9   4.98  ,\n",
       " 0    24.0\n",
       " Name: price, dtype: float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TabularData(df, 'price')[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularData(df, 'price')\n",
    "data = DataLoader(data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (w) Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (w) Language processing\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/text/stable/index.html>torchtext</a></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (w) Image processing\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/vision/stable/index.html>torchvision</a></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (w) Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has two APIs for creating layers,\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/nn.html>nn</a></code>\n",
    "(abbreviated for *neural network*) and\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/nn.functional.html>nn.functional</a></code>.\n",
    "The first module provides object interface (that supports auto differentiation) and the second module provides function interface (easier to use). So, the best practice is using object interface for layers with trainable parameters such as recurrent or convolutional, and using function interface for loss functions or activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has two APIs for creating models, where the recommended one is\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/generated/torch.nn.Module.html>nn.Module</a></code>,\n",
    "being equivalent to functional API in Keras. To create a model, we inherit this class, define building blocks inside the <code style='font-size:13px'>\\_\\_init__()</code> method and design the neural network architecture with the <code style='font-size:13px'>foward()</code> method. We don't need to to specify the backward pass, as the submodule\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/autograd.html>autograd</a></code>\n",
    "will handle it for us. The second API,\n",
    "<code style='font-size:13px'><a href=https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>nn.Sequential</a></code>,\n",
    "is good for simple architectures as well as small blocks of large networks, inception block of GooLeNet for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([[1,2,3,4,5]]), requires_grad=True)\n",
    "y = torch.sum(x**2)\n",
    "y.backward() \n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "\n",
    "        D_in: input dimension\n",
    "        H: dimension of hidden layer\n",
    "        D_out: output dimension\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H) \n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "\n",
    "def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must \n",
    "        return a Variable of output data. We can use Modules defined in the \n",
    "        constructor as well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        h_relu = nn.functional.relu(self.linear1(x))\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle:\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "    def area(self):\n",
    "        return self.length * self.width\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * self.length + 2 * self.width\n",
    "\n",
    "class Square(Rectangle):\n",
    "    def __init__(self, length):\n",
    "        super().__init__(length, length)\n",
    "        \n",
    "class Cube(Square):\n",
    "    def surface_area(self):\n",
    "        face_area = super().area()\n",
    "        return face_area * 6\n",
    "\n",
    "    def volume(self):\n",
    "        face_area = super().area()\n",
    "        return face_area * self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- *pytorch.org - [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html)*\n",
    "- *pytorch.org - [Automatic differentiation with Torch.Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)*\n",
    "- *pytorch.org - [Deep Learning with PyTorch: A 60-minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)*\n",
    "- *towardsdatascience.com - [Understanding PyTorch with an example: a step-by-step tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)*\n",
    "- *towardsdatascience.com - [PyTorch vs TensorFlow - spotting the difference](https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)*\n",
    "- *blog.paperspace.com - [PyTorch 101 advanced](https://blog.paperspace.com/pytorch-101-advanced/)*\n",
    "- *poloclub.github.io - [CNN explainer](https://poloclub.github.io/cnn-explainer/)*\n",
    "- https://cs230.stanford.edu/blog/pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
