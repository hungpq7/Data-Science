{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recurrent layers\n",
    "[Recurrent Neural Network] (RNN) is a class of neural network architectures where nodes in a layers have internal connections, allowing to express temporal behaviour. There are many types of RNN layers, but they all share the same architecture. The image below shows the information flow for an observation, or for a document in the context of NLP.\n",
    "\n",
    "<img src='image/rnn_general.png' style='height:175px; margin:20px auto;'>\n",
    "\n",
    "Each green cell $\\mathbf{x}_t\\in\\mathbb{R}^{V\\times1}$ represents the embedding vector of a token, and each blue cell $\\mathbf{h}_t\\in\\mathbb{R}^{D\\times1}$ represents an output vector. With the input sequence size is fixed at $T$, RNN adjusts itself to match the input length. The most important part of a RNN layer is the grey cell $A$ that repeats multiple times, being account for information processing. We can see that at a time step, the output value $\\mathbf{h}_t$ is influenced by all previous steps $\\mathbf{h}_{t-1},\\mathbf{h}_{t-2},\\dots$, besides the input $\\mathbf{x}_t$. This design resembles *memory* and enables RNN to capture sequential relationship.\n",
    "\n",
    "There are many architectures for a recurrent layers, the only difference between them is how the cell $A$ being desgined. In this article, we are going to learn the cell architectures of Simple RNN, LSTM and GRU.\n",
    "\n",
    "[Recurrent Neural Network]: https://en.wikipedia.org/wiki/Recurrent_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Common blocks\n",
    "This section introduces common blocks in recurrent architectures. Knowing each of them separately helps us understanding compicated designs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "<img src='image/rnn_concatenation.png' style='height:100px; margin:0px auto;'>\n",
    "\n",
    "Let's say we want to transform two input vectors $\\mathbf{u}\\in\\mathbb{R}^{U\\times1}$ and $\\mathbf{v}\\in\\mathbb{R}^{V\\times1}$ into $\\mathbf{y}\\in\\mathbb{R}^{D\\times1}$. Note that $U$ and $V$ are fixed dimensionalities of input, while $D$ is the desired output size. With weight matrices\n",
    "$\\mathbf{W}_{yu}\\in\\mathbb{R}^{D\\times U},\\mathbf{W}_{yv}\\in\\mathbb{R}^{D\\times V}$\n",
    "and bias vector $\\mathbf{b}_y\\in\\mathbb{R}^{D\\times1}$,\n",
    "the actual formula behind the above image is:\n",
    "\n",
    "$$\\mathbf{y}=\\mathbf{W}_{yu}\\mathbf{u}+\\mathbf{W}_{yv}\\mathbf{v}+\\mathbf{b}_y$$\n",
    "\n",
    "Here, all three terms have size $(D\\times1)$, same as $\\mathbf{y}$. We can also view the above formula as concatenating $\\mathbf{u}$ and $\\mathbf{v}$ into a single input vector $\\mathbf{x}\\in\\mathbb{R}^{(U+V)\\times1}$, then scale it using a bigger weight matrix $\\mathbf{W}_{yx}\\in\\mathbb{R}^{D\\times(U+V)}$. This explains why the formula is visualized as a concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate\n",
    "\n",
    "<img src='image/rnn_gate.png' style='height:80px; margin:0px auto;'>\n",
    "\n",
    "A gate consists of two calculation steps, (1) passing a vector into sigmoid function and (2) using it as a percentage multiplier. The sigmoid function (denoted $\\sigma$) is account for producing numbers in range $(0,1)$. We can see the purpose of gates very clearly here: they control how much information should be let through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "We call the vanilla architecture [Simple RNN] (1980s) to distinguish from the family name. Its cells is very simple, with only a concatenated value pass through an activation function. The activation function is usually $\\tanh$ which produces values within the range $(-1,1)$, so that the network will be able to express *sentiment*. The cell architecture is described in the image and formula as follows:\n",
    "\n",
    "<img src='image/rnn_cell.png' style='height:160px; margin:0px auto;'>\n",
    "\n",
    "$$\\mathbf{h}_t=\\phi(\\mathbf{W}_{hx}\\mathbf{x}_t+\\mathbf{W}_{hh}\\mathbf{h}_{t-1}+\\mathbf{b}_h)$$\n",
    "\n",
    "$\\mathbf{W}_{hx},\\mathbf{W}_{hh}$ and $\\mathbf{b}_h$, as explained earlier, are the weight matrices and bias vector. Their corresponding sizes are $(D\\times V)$, $(D\\times D)$ and $(D\\times 1)$. Note that these parameters are used across cells, hence taking sum of their sizes gets us the total number of parameters need to be trained. For example, we use a BERT pretrained model to encode a corpus containing $N=10\\,000$ documents. The embedding dimension is $V=512$ and documents are truncated to have $T=128$ tokens. If we set the number of units of the RNN layer to $D=50$, then the number of parameters our network has is $D\\times(D+V+1)=8950$.\n",
    "\n",
    "A well-known issue with Simple RNN is that it only has *short-term memory*. This property is very easy to understand if you are familiar with the gradient vanishing problem of S-shaped activation functions. During [backpropagation through time] for a pair of words with large $\\Delta t$, the product of partial derivatives may trigger saturation zones of $\\tanh$, making the derivative of a word with respect to the other almost zero. As a result, Simple RNN fails to capture long-term memory.\n",
    "\n",
    "[Simple RNN]: https://en.wikipedia.org/wiki/Recurrent_neural_network#Fully_recurrent\n",
    "[backpropagation through time]: https://en.wikipedia.org/wiki/Backpropagation_through_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "RNN is implemented in TensorFlow in both layer-level and cell-level, via the classes\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN>SimpleRNN</a></code>\n",
    "and\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNNCell>SimpleRNNCell</a></code>.\n",
    "They have the following hyperparameters:\n",
    "- <code style='font-size:13px; color:firebrick'>units</code>: the dimensionality of output space ($D$).\n",
    "- <code style='font-size:13px; color:firebrick'>activation</code>: the activation function used in cells, defaults to *tanh*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style='color:navy'><i class=\"fa fa-info-circle\"></i>&nbsp; Note</b><br>\n",
    "When using RNN with other layers, there are two cases:\n",
    "- The next layer being Fully Connected, then we only use the last hidden state, $\\mathbf{h}_T$. The output shape in this case is  $(N\\times D)$.\n",
    "- The next layer being another RNN layer (including LSTM and GRU), then we need to return the full sequence $\\mathbf{h}_1,\\mathbf{h}_2,\\dots,\\mathbf{h}_T$. This is done by specifying <code style='font-size:13px'>return_sequences=True</code>. The output shape this time is $(N\\times T\\times D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T16:45:37.236157Z",
     "iopub.status.busy": "2022-12-16T16:45:37.235699Z",
     "iopub.status.idle": "2022-12-16T16:45:45.242017Z",
     "shell.execute_reply": "2022-12-16T16:45:45.241433Z",
     "shell.execute_reply.started": "2022-12-16T16:45:37.236023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sspipe import p, px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T16:51:24.183041Z",
     "iopub.status.busy": "2022-12-16T16:51:24.181989Z",
     "iopub.status.idle": "2022-12-16T16:51:24.190361Z",
     "shell.execute_reply": "2022-12-16T16:51:24.189007Z",
     "shell.execute_reply.started": "2022-12-16T16:51:24.182990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.random.random((32,10,8))\n",
    "y = np.random.random((32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T16:51:25.028545Z",
     "iopub.status.busy": "2022-12-16T16:51:25.028178Z",
     "iopub.status.idle": "2022-12-16T16:51:25.251177Z",
     "shell.execute_reply": "2022-12-16T16:51:25.249923Z",
     "shell.execute_reply.started": "2022-12-16T16:51:25.028520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 10, 5)             70        \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 3)                 27        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137\n",
      "Trainable params: 137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.SimpleRNN(5, input_shape=(10,8), return_sequences=True),\n",
    "    layers.SimpleRNN(3),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'simple_rnn_15/simple_rnn_cell_15/kernel:0' shape=(8, 4) dtype=float32, numpy=\n",
       " array([[-0.12565374, -0.17764747,  0.19202441,  0.00790167],\n",
       "        [ 0.42867404,  0.17833441, -0.6105001 ,  0.49296635],\n",
       "        [-0.40300983,  0.3589633 , -0.26932156, -0.41699216],\n",
       "        [-0.28938767,  0.16760606, -0.34825772,  0.5725295 ],\n",
       "        [ 0.3798756 , -0.45982867,  0.30792862, -0.05597204],\n",
       "        [ 0.39521652,  0.21937352,  0.3404147 ,  0.5420316 ],\n",
       "        [ 0.37472934,  0.37950474, -0.04657042,  0.18962324],\n",
       "        [ 0.37935108,  0.24689758, -0.4418146 , -0.25006822]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'simple_rnn_15/simple_rnn_cell_15/recurrent_kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[-0.80420995, -0.04441664,  0.47590402,  0.35325453],\n",
       "        [ 0.07364351, -0.4747402 ,  0.57299924, -0.6639806 ],\n",
       "        [-0.29404065,  0.7626039 ,  0.0019677 , -0.57616967],\n",
       "        [-0.51123667, -0.4371317 , -0.66722065, -0.31995237]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'simple_rnn_15/simple_rnn_cell_15/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T16:51:33.405332Z",
     "iopub.status.busy": "2022-12-16T16:51:33.404768Z",
     "iopub.status.idle": "2022-12-16T16:51:33.411987Z",
     "shell.execute_reply": "2022-12-16T16:51:33.410281Z",
     "shell.execute_reply.started": "2022-12-16T16:51:33.405288Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5)\n",
      "(5, 5)\n",
      "(5,)\n",
      "(5, 3)\n",
      "(3, 3)\n",
      "(3,)\n",
      "(3, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "_ = [print(weight.shape) for weight in model.weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.6159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26f277c7ee0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T17:29:38.066596Z",
     "iopub.status.busy": "2022-12-16T17:29:38.066082Z",
     "iopub.status.idle": "2022-12-16T17:29:38.096967Z",
     "shell.execute_reply": "2022-12-16T17:29:38.096175Z",
     "shell.execute_reply.started": "2022-12-16T17:29:38.066555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfSpam = pd.read_csv('data/spam_message.csv')\n",
    "dfSpam.sample(frac=1, random_state=5)\n",
    "dfTrain = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T17:31:22.581402Z",
     "iopub.status.busy": "2022-12-16T17:31:22.581052Z",
     "iopub.status.idle": "2022-12-16T17:31:22.596758Z",
     "shell.execute_reply": "2022-12-16T17:31:22.595322Z",
     "shell.execute_reply.started": "2022-12-16T17:31:22.581379Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>0</td>\n",
       "      <td>Probably, want to pick up more?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5343</th>\n",
       "      <td>0</td>\n",
       "      <td>No go. No openings for that room 'til after th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>Fuck babe ... I miss you already, you know ? C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>0</td>\n",
       "      <td>I to am looking forward to all the sex cuddlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm freezing and craving ice. Fml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok. Not much to do here though. H&amp;M Friday, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>0</td>\n",
       "      <td>You know there is. I shall speak to you in  &amp;l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0</td>\n",
       "      <td>Sir, good morning. Hope you had a good weekend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok. Me watching tv too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>0</td>\n",
       "      <td>What time should I tell my friend to be around?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam                                            content\n",
       "2095     0                    Probably, want to pick up more?\n",
       "5343     0  No go. No openings for that room 'til after th...\n",
       "564      0  Fuck babe ... I miss you already, you know ? C...\n",
       "3849     0  I to am looking forward to all the sex cuddlin...\n",
       "3317     0                  I'm freezing and craving ice. Fml\n",
       "...    ...                                                ...\n",
       "3046     0  Ok. Not much to do here though. H&M Friday, ca...\n",
       "1725     0  You know there is. I shall speak to you in  &l...\n",
       "4079     0  Sir, good morning. Hope you had a good weekend...\n",
       "2254     0                            Ok. Me watching tv too.\n",
       "2915     0    What time should I tell my friend to be around?\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSpam.sample(frac=1, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T17:29:40.815812Z",
     "iopub.status.busy": "2022-12-16T17:29:40.815422Z",
     "iopub.status.idle": "2022-12-16T17:29:40.839295Z",
     "shell.execute_reply": "2022-12-16T17:29:40.838355Z",
     "shell.execute_reply.started": "2022-12-16T17:29:40.815784Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam                                            content\n",
       "0        0  Go until jurong point, crazy.. Available only ...\n",
       "1        0                      Ok lar... Joking wif u oni...\n",
       "2        1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3        0  U dun say so early hor... U c already then say...\n",
       "4        0  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567     1  This is the 2nd time we have tried 2 contact u...\n",
       "5568     0              Will Ì_ b going to esplanade fr home?\n",
       "5569     0  Pity, * was in mood for that. So...any other s...\n",
       "5570     0  The guy did some bitching but I acted like i'd...\n",
       "5571     0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T17:28:13.113512Z",
     "iopub.status.busy": "2022-12-16T17:28:13.112960Z",
     "iopub.status.idle": "2022-12-16T17:28:17.911743Z",
     "shell.execute_reply": "2022-12-16T17:28:17.910389Z",
     "shell.execute_reply.started": "2022-12-16T17:28:13.113471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bertProcessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bertEncoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T17:15:31.358788Z",
     "iopub.status.busy": "2022-12-16T17:15:31.358442Z",
     "iopub.status.idle": "2022-12-16T17:15:31.378612Z",
     "shell.execute_reply": "2022-12-16T17:15:31.376602Z",
     "shell.execute_reply.started": "2022-12-16T17:15:31.358764Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"packer\" (type KerasLayer).\n\nCould not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (3 total):\n    * ['this is such an amazing movie', 'this movie is terrible']\n    * False\n    * None\n  Keyword arguments: {'seq_length': 16}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n    * True\n    * None\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n    * True\n    * None\n  Keyword arguments: {}\n\nCall arguments received:\n  • inputs=[\"'this is such an amazing movie'\", \"'this movie is terrible'\"]\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4daf5b4982fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m       result = smart_cond.smart_cond(training,\n\u001b[0m\u001b[1;32m    238\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                                      lambda: f(training=False))\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    238\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"packer\" (type KerasLayer).\n\nCould not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (3 total):\n    * ['this is such an amazing movie', 'this movie is terrible']\n    * False\n    * None\n  Keyword arguments: {'seq_length': 16}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n    * True\n    * None\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (3 total):\n    * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n    * True\n    * None\n  Keyword arguments: {}\n\nCall arguments received:\n  • inputs=[\"'this is such an amazing movie'\", \"'this movie is terrible'\"]\n  • training=None"
     ]
    }
   ],
   "source": [
    "doc = [\n",
    "    'this is such an amazing movie',\n",
    "    'this movie is terrible',\n",
    "]\n",
    "\n",
    "doc = bertProcessor(doc)\n",
    "embed = bertEncoder(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T17:01:03.146290Z",
     "iopub.status.busy": "2022-12-16T17:01:03.145946Z",
     "iopub.status.idle": "2022-12-16T17:01:03.155250Z",
     "shell.execute_reply": "2022-12-16T17:01:03.154507Z",
     "shell.execute_reply.started": "2022-12-16T17:01:03.146267Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed['pooled_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T17:01:04.327732Z",
     "iopub.status.busy": "2022-12-16T17:01:04.327391Z",
     "iopub.status.idle": "2022-12-16T17:01:04.331897Z",
     "shell.execute_reply": "2022-12-16T17:01:04.331171Z",
     "shell.execute_reply.started": "2022-12-16T17:01:04.327709Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 128, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed['sequence_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed['pooled_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 128, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed['sequence_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. LSTM\n",
    "[LSTM] (Long Short-Term Memory, 1997)\n",
    "\n",
    "<img src='image/lstm_cell.png' style='height:320px; margin:20px auto;'>\n",
    "\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM>LSTM</a></code>\n",
    "\n",
    "[LSTM]: https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image/lstm_steps.png' style='height:520px; margin:20px auto;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. GRU\n",
    "[Gated Recurrent Units] (GRU)\n",
    "\n",
    "<img src='image/gru_cell.png' style='height:320px; margin:20px auto;'>\n",
    "\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU>GRU</a></code>\n",
    "\n",
    "[Gated Recurrent Units]: https://en.wikipedia.org/wiki/Gated_recurrent_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Bi-directional\n",
    "\n",
    "<img src='image/rnn_bidirectional.png' style='height:245px; margin:20px auto;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recurrent architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Seq2seq\n",
    "[Seq2seq]\n",
    "\n",
    "[Seq2seq]: https://en.wikipedia.org/wiki/Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Attention\n",
    "[Attention] implement\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention>Attention</a></code>\n",
    "\n",
    "[Attention]: https://en.wikipedia.org/wiki/Attention_(machine_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Transformer\n",
    "[Transformer]\n",
    "\n",
    "[Transformer]: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- *amitness.com - [Recurrent Keras layer](https://amitness.com/2020/04/recurrent-layers-keras/)*\n",
    "- *colah.github.io - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*\n",
    "- *d2l.ai - [Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)*\n",
    "- *d2l.ai - [Long Short-Term Memory](https://d2l.ai/chapter_recurrent-modern/lstm.html)*\n",
    "- *d2l.ai - [Gated Recurrent Units](https://d2l.ai/chapter_recurrent-modern/gru.html)*\n",
    "- *distill.pub - [Memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)*\n",
    "- *distill.pub - [Augumented RNNs](https://distill.pub/2016/augmented-rnns/)*\n",
    "---\n",
    "- https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "- https://www.kaggle.com/code/kredy10/simple-lstm-for-text-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
