{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Syntactic processing\n",
    "\n",
    "Syntactic analysis or parsing is defined as the process of analyzing the strings of symbols in natural language conforming to the rules of formal grammar. The purpose of this process is to draw exact meaning, or perform dictionary meaning from the text. Syntax analysis checks the text for meaningfulness comparing to the rules of formal grammar. Example:\n",
    "1. Delhi is the capital of India.\n",
    "2. Is Delhi the of India capital.\n",
    "\n",
    "Two sentences have the same word but only sentence 1 was meaningful and syntactically correct. The purpose of sytactic processing is recover the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Part of speech (POS) tagging \n",
    "\n",
    "Part of speech is the process  which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context. To analyze the relationship and understanding meaning of text, pos tagging is very important process. POS tag are useful for building parse trees, which are used in building NERs and extracting relations between words. It is also use for building lemmatizers in 1.3 \n",
    "\n",
    "Some pos tagging techniques:\n",
    "- Lexical base method: Assigns the POS tag the most frequently occurring with a word in the training corpus\n",
    "- Rule based method: Assigns POS tags based on rules in dictionary.\n",
    "- Probabilistic method: This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.\n",
    "- Deep learning method: Recurrent Neural Networks can also be used for POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Random Fields (CRFs)\n",
    "\n",
    "Conditional Random Fields are a discriminative model, used for predicting sequences. They use contextual information from previous labels, thus increasing the amount of information the model has to make a good prediction.\n",
    "Discriminative classifier - they model the decision boundary between the different classes (just like logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Parsing\n",
    "One of the most important parts of syntactic processing is parsing. It means to break down a given sentence into its *grammatical components*. NLTK doesn't support pre-trained English grammar model, we have to manually specify grammar before parsing a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NLP for Vietnamese language \n",
    "\n",
    "With the Vietnamese language, the packages haven't developed completely and they don't have specific documentation. Some common packages are pyvi, vncorenlp and underthesea. These packages provide some basic function such as word tokenize, pos tagging and removing accent. There is no quite difference between pyvi and underthesea despite of some pre-trained models that underthesea provides like NER, classify and sentiment analysis\n",
    "\n",
    "*Reference:* \n",
    "[Pyvi](https://pypi.org/project/pyvi/) and \n",
    "[Underthesea](https://underthesea.readthedocs.io/en/latest/readme.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger, ViUtils,ViDiac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hồ', 'gươm', 'là', 'danh_lam', 'thắng_cảnh', 'Hà_Nội']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTokenizer.tokenize(\"Hồ gươm là danh lam thắng cảnh Hà Nội\").split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tôi', 'là', 'sinh_viên', 'trường', 'cao_đẳng', 'y_tế', 'hà_tây']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_token, _ = ViTokenizer.spacy_tokenize(\"Tôi là sinh viên trường cao đẳng y tế hà tây\")\n",
    "list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hồ', 'gươm', 'là', 'danh', 'lam', 'thắng', 'cảnh', 'Hà', 'Nội'],\n",
       " ['N', 'N', 'V', 'N', 'N', 'V', 'N', 'Np', 'Np'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViPosTagger.postagging(\"Hồ gươm là danh lam thắng cảnh Hà Nội\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ho guom la danh lam thang canh Ha Noi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViUtils.remove_accents(u\"Hồ gươm là danh lam thắng cảnh Hà Nội\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another way to remove accent is using unidecode packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ho guom la danh lam thang canh Ha Noi'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "unidecode.unidecode('Hồ gươm là danh lam thắng cảnh Hà Nội')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "from underthesea import sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Với xử lí tiếng việt, các thư viện chưa phát triển nhiều.',\n",
       " 'Một số thư viện phổ biến là pyvi và underthesea']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Với xử lí tiếng việt, các thư viện chưa phát triển nhiều. Một số thư viện phổ biến là pyvi và underthesea\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Với', 'xử lí', 'tiếng', 'việt', ',', 'các', 'thư viện', 'chưa', 'phát triển', 'nhiều', '.', 'Một số', 'thư viện', 'phổ biến', 'là', 'pyvi', 'và', 'underthesea']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Với', 'E'),\n",
       " ('xử lí', 'N'),\n",
       " ('tiếng', 'N'),\n",
       " ('việt', 'V'),\n",
       " (',', 'CH'),\n",
       " ('các', 'L'),\n",
       " ('thư viện', 'N'),\n",
       " ('chưa', 'R'),\n",
       " ('phát triển', 'V'),\n",
       " ('nhiều', 'A'),\n",
       " ('.', 'CH'),\n",
       " ('Một số', 'L'),\n",
       " ('thư viện', 'N'),\n",
       " ('phổ biến', 'V'),\n",
       " ('là', 'V'),\n",
       " ('pyvi', 'N'),\n",
       " ('và', 'C'),\n",
       " ('underthesea', 'M')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Regex \n",
    "\n",
    "Some string processing techniques with regex was introduce in `2. [python] Classes`. Therefore, below will only introduce some common patterns in text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🔥', '𝑩', '𝑨', '𝑪', '𝑲', '𝑻', '𝑶', '𝑺', '𝑪', '𝑯', '𝑶', '𝑶', '𝑳', 'J', '™', '💃', '\\U0001fab5', '\\U0001fab5', '【', 'J', 'N', '】', 'K', '🌈', '𝗡', '𝗘', '𝗪', '𝗔', '𝗥', '𝗥', '𝗜', '𝗩', '𝗔', '𝗟', '💢', '📽', '️', '9', '2', 'Q', '🔵', '𝐅', '𝐑', '𝐄', '𝐄', '𝐒', '𝐇', '𝐈', '𝐏', '🔵']\n"
     ]
    }
   ],
   "source": [
    "# find special characters\n",
    "pattern ='[^aàảãáạăằẳẵắặâầẩẫấậbcdđeèẻẽéẹêềểễếệfghiìỉĩíịjklmnoòỏõóọôồổỗốộơờởỡớợpqrstuùủũúụưừửữứựvwxyỳỷỹýỵz\\s]'\n",
    "string = \"\"\"🔥𝑩𝑨𝑪𝑲 𝑻𝑶 𝑺𝑪𝑯𝑶𝑶𝑳 balo Japan classic™   balo đi học  balo laptop   balo thời trang   balo chống nước\n",
    "💃 đầm trắng nữ cổ vuông eo chun váy nữ cộc tay chất đũi dáng xòe\n",
    "🪵có sẵn set áo babydoll thô đũi viền ren kèm quần đùi 🪵\n",
    "【JN】heybig spring and summer new Korean\n",
    "🌈𝗡𝗘𝗪 𝗔𝗥𝗥𝗜𝗩𝗔𝗟💢 áo khoác kaki unisex 📽️ videoảnh thật a92\n",
    "Quần jean nam trơn màu xanh 🔵 𝐅𝐑𝐄𝐄 𝐒𝐇𝐈𝐏 🔵 quần bò nam co giãn thời trang hpfashion\"\"\"\n",
    "\n",
    "print(re.findall(pattern,string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'I ', ' '), (' ', 'a ', ' ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find stop word\n",
    "pattern = '(^|\\s+)(\\S(\\s+|$))+'\n",
    "sen = 'I need a doctor'\n",
    "\n",
    "re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://regex101.com/']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find url link\n",
    "pattern = 'http\\S+'\n",
    "sen = 'Reference: https://regex101.com/ (regex online checking)'\n",
    "\n",
    "re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find number of episol\n",
    "# pattern = '(?<=phần\\s|tập\\s|t|t.)\\d+'\n",
    "# sen = \"\"\"thiên thần 1001 tập 19 \n",
    "# thiên thần 1001 tập 18 \n",
    "# phim trung quốc: hán sở tranh hùng-t.85 \n",
    "# phim trung quốc: hán sở tranh hùng-t86 \n",
    "# vụ án ngay bên bạn: bộ hài cốt bí ẩn-phần 7\"\"\"\n",
    "\n",
    "# re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.google', 'id.zalo']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find sub domain of url\n",
    "pattern = '(?<=//)\\S+(?=\\.)'\n",
    "sen = \"\"\"https://www.google.ca/\n",
    "https://id.zalo.me/account/outapp\"\"\"\n",
    "\n",
    "re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20211021121219895', '20211021185707803']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find item id in a url\n",
    "pattern = '\\d+(?=rf\\d+|\\.htm)'\n",
    "url1 = 'https://soha.vn/giam-doc-bv-bach-mai-nguyen-quang-tuan-bi-khoi-to-bo-y-te-noi-gi-20211021185707803.htm'\n",
    "url2 = 'https://soha.vn/phu-tho-ghi-nhan-them-17-ca-duong-tinh-voi-sars-cov-2-20211021121219895rf20211021185707803.htm'\n",
    "re.findall(pattern, url1)\n",
    "re.findall(pattern, url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
