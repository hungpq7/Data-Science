{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recurrent layers\n",
    "[Recurrent Neural Network] (RNN) is a class of neural network architectures where nodes in a layers have internal connections, allowing to express temporal behaviour. There are many types of RNN layers, but they all share the same architecture. The image below shows the information flow for an observation, or for a document in the context of NLP.\n",
    "\n",
    "<img src='image/rnn_general.png' style='height:175px; margin:20px auto;'>\n",
    "\n",
    "Each green cell $\\mathbf{x}_t\\in\\mathbb{R}^{V\\times1}$ represents the embedding vector of a token, and each blue cell $\\mathbf{h}_t\\in\\mathbb{R}^{D\\times1}$ represents an output vector. With the input sequence size is fixed at $T$, RNN adjusts itself to match the input length. The most important part of a RNN layer is the grey cell $A$ that repeats multiple times, being account for information processing. We can see that at a time step, the output value $\\mathbf{h}_t$ is influenced by all previous steps $\\mathbf{h}_{t-1},\\mathbf{h}_{t-2},\\dots$, besides the input $\\mathbf{x}_t$. This design resembles *memory* and enables RNN to capture sequential relationship.\n",
    "\n",
    "There are many architectures for a recurrent layers, the only difference between them is how the cell $A$ being desgined. In this article, we are going to learn the cell architectures of Simple RNN, LSTM and GRU.\n",
    "\n",
    "[Recurrent Neural Network]: https://en.wikipedia.org/wiki/Recurrent_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Common blocks\n",
    "This section introduces common blocks in recurrent architectures. Knowing each of them separately helps us understanding compicated designs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "<img src='image/rnn_concatenation.png' style='height:100px; margin:0px auto;'>\n",
    "\n",
    "Let's say we want to transform two input vectors $\\mathbf{u}\\in\\mathbb{R}^{U\\times1}$ and $\\mathbf{v}\\in\\mathbb{R}^{V\\times1}$ into $\\mathbf{y}\\in\\mathbb{R}^{D\\times1}$. Note that $U$ and $V$ are fixed dimensionalities of input, while $D$ is the desired output size. With weight matrices\n",
    "$\\mathbf{W}_{yu}\\in\\mathbb{R}^{D\\times U},\\mathbf{W}_{yv}\\in\\mathbb{R}^{D\\times V}$\n",
    "and bias vector $\\mathbf{b}_y\\in\\mathbb{R}^{D\\times1}$,\n",
    "the actual formula behind the above image is:\n",
    "\n",
    "$$\\mathbf{y}=\\mathbf{W}_{yu}\\mathbf{u}+\\mathbf{W}_{yv}\\mathbf{v}+\\mathbf{b}_y$$\n",
    "\n",
    "Here, all three terms have size $(D\\times1)$, same as $\\mathbf{y}$. We can also view the above formula as concatenating $\\mathbf{u}$ and $\\mathbf{v}$ into a single input vector $\\mathbf{x}\\in\\mathbb{R}^{(U+V)\\times1}$, then scale it using a bigger weight matrix $\\mathbf{W}_{yx}\\in\\mathbb{R}^{D\\times(U+V)}$. This explains why the formula is visualized as a concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate\n",
    "\n",
    "<img src='image/rnn_gate.png' style='height:80px; margin:0px auto;'>\n",
    "\n",
    "A gate consists of two calculation steps, (1) passing a vector into sigmoid function and (2) using it as a percentage multiplier. The sigmoid function (denoted $\\sigma$) is account for producing numbers in range $(0,1)$. We can see the purpose of gates very clearly here: they control how much information should be let through."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Simple RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "We call the vanilla architecture [Simple RNN] (1980s) to distinguish from the family name. Its cells is very simple, with only a concatenated value pass through an activation function. The activation function is usually $\\tanh$ which produces values within the range $(-1,1)$, so that the network will be able to express *sentiment*. The cell architecture is described in the image and formula as follows:\n",
    "\n",
    "<img src='image/rnn_cell.png' style='height:160px; margin:0px auto;'>\n",
    "\n",
    "$$\\mathbf{h}_t=\\phi(\\mathbf{W}_{hx}\\mathbf{x}_t+\\mathbf{W}_{hh}\\mathbf{h}_{t-1}+\\mathbf{b}_h)$$\n",
    "\n",
    "A well-known issue with Simple RNN is that it only has *short-term memory*. This property is very easy to understand if you are familiar with the gradient vanishing problem of S-shaped activation functions. During [backpropagation through time] for a pair of words with large $\\Delta t$, the product of partial derivatives may trigger saturation zones of $\\tanh$, making the derivative of a word with respect to the other almost zero. As a result, Simple RNN fails to capture long-term memory.\n",
    "\n",
    "[Simple RNN]: https://en.wikipedia.org/wiki/Recurrent_neural_network#Fully_recurrent\n",
    "[backpropagation through time]: https://en.wikipedia.org/wiki/Backpropagation_through_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing\n",
    "$\\mathbf{W}_{hx},\\mathbf{W}_{hh}$ and $\\mathbf{b}_h$, as explained earlier, are the weight matrices and bias vector. Their corresponding sizes are $(D\\times V)$, $(D\\times D)$ and $(D\\times 1)$. Note that these parameters are used across cells, hence taking sum of their sizes gets us the total number of parameters need to be trained:\n",
    "\n",
    "$$D\\times(D+V+1)$$\n",
    "\n",
    "For example, we use a BERT pretrained model to encode a corpus containing $N=10\\,000$ documents. The embedding dimension is $V=512$ and documents are truncated to have $T=128$ tokens. Then a RNN layer with $D=50$ will require $50\\times(50+512+1)=28\\,150$ parameters to process such input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "RNN is implemented in TensorFlow in both layer-level and cell-level, via the classes\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN>SimpleRNN</a></code>\n",
    "and\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNNCell>SimpleRNNCell</a></code>.\n",
    "They have the following hyperparameters:\n",
    "- <code style='font-size:13px; color:firebrick'>units</code>: the dimensionality of output space ($D$).\n",
    "- <code style='font-size:13px; color:firebrick'>activation</code>: the activation function ($\\phi$), defaults to *tanh*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:54:38.438778Z",
     "iopub.status.busy": "2022-12-17T16:54:38.438157Z",
     "iopub.status.idle": "2022-12-17T16:54:45.256173Z",
     "shell.execute_reply": "2022-12-17T16:54:45.255034Z",
     "shell.execute_reply.started": "2022-12-17T16:54:38.438677Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sspipe import p, px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:54:45.257865Z",
     "iopub.status.busy": "2022-12-17T16:54:45.257460Z",
     "iopub.status.idle": "2022-12-17T16:54:45.274689Z",
     "shell.execute_reply": "2022-12-17T16:54:45.273178Z",
     "shell.execute_reply.started": "2022-12-17T16:54:45.257841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfSpam = pd.read_csv('data/spam_message.csv')\n",
    "xTrainRaw, xTestRaw, yTrain, yTest = train_test_split(dfSpam.content, dfSpam.spam, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:54:45.276935Z",
     "iopub.status.busy": "2022-12-17T16:54:45.276538Z",
     "iopub.status.idle": "2022-12-17T16:55:55.791196Z",
     "shell.execute_reply": "2022-12-17T16:55:55.790387Z",
     "shell.execute_reply.started": "2022-12-17T16:54:45.276896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_feature(corpus):\n",
    "    bertProcessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "    bertEncoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/2')\n",
    "    corpus = corpus.str.lower()\n",
    "    corpus = bertProcessor(corpus)\n",
    "    corpus = bertEncoder(corpus)\n",
    "    corpus = corpus['sequence_output']\n",
    "    return corpus\n",
    "    \n",
    "xTrain = process_feature(xTrainRaw)\n",
    "xTest = process_feature(xTestRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:07:22.161985Z",
     "iopub.status.busy": "2022-12-17T03:07:22.161562Z",
     "iopub.status.idle": "2022-12-17T03:07:22.170452Z",
     "shell.execute_reply": "2022-12-17T03:07:22.169486Z",
     "shell.execute_reply.started": "2022-12-17T03:07:22.161941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 128, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:18:51.161934Z",
     "iopub.status.busy": "2022-12-17T03:18:51.161550Z",
     "iopub.status.idle": "2022-12-17T03:18:51.595588Z",
     "shell.execute_reply": "2022-12-17T03:18:51.594860Z",
     "shell.execute_reply.started": "2022-12-17T03:18:51.161911Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.SimpleRNN(16)(xTrain).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T03:19:05.115829Z",
     "iopub.status.busy": "2022-12-17T03:19:05.115469Z",
     "iopub.status.idle": "2022-12-17T03:19:05.455679Z",
     "shell.execute_reply": "2022-12-17T03:19:05.454843Z",
     "shell.execute_reply.started": "2022-12-17T03:19:05.115805Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4457, 128, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.SimpleRNN(16, return_sequences=True)(xTrain).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style='color:navy'><i class=\"fa fa-info-circle\"></i>&nbsp; Note</b><br>\n",
    "When using RNN with other layers, there are two cases:\n",
    "- The next layer being Fully Connected, then we only use the last hidden state, $\\mathbf{h}_T$. The output shape in this case is  $(N\\times D)$.\n",
    "- The next layer being another RNN layer (including LSTM and GRU), then we need to return the full sequence $\\mathbf{h}_1,\\mathbf{h}_2,\\dots,\\mathbf{h}_T$. This is done by specifying <code style='font-size:13px'>return_sequences=True</code>. The output shape this time is $(N\\times T\\times D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:11.976104Z",
     "iopub.status.busy": "2022-12-17T16:56:11.975625Z",
     "iopub.status.idle": "2022-12-17T16:56:16.846199Z",
     "shell.execute_reply": "2022-12-17T16:56:16.845320Z",
     "shell.execute_reply.started": "2022-12-17T16:56:11.976074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 5s 25ms/step - loss: 0.0793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9134fe7f40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.SimpleRNN(10, return_sequences=True),\n",
    "    layers.SimpleRNN(5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:25.009641Z",
     "iopub.status.busy": "2022-12-17T16:56:25.009088Z",
     "iopub.status.idle": "2022-12-17T16:56:25.031197Z",
     "shell.execute_reply": "2022-12-17T16:56:25.030317Z",
     "shell.execute_reply.started": "2022-12-17T16:56:25.009597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 128, 10)           2670      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 5)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,756\n",
      "Trainable params: 2,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:56:37.708607Z",
     "iopub.status.busy": "2022-12-17T16:56:37.708278Z",
     "iopub.status.idle": "2022-12-17T16:56:38.266645Z",
     "shell.execute_reply": "2022-12-17T16:56:38.266043Z",
     "shell.execute_reply.started": "2022-12-17T16:56:37.708585Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789832918592251"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTestPred = model.predict(xTest)\n",
    "AUC(yTest, yTestPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-17T16:58:19.396320Z",
     "iopub.status.busy": "2022-12-17T16:58:19.395930Z",
     "iopub.status.idle": "2022-12-17T16:58:19.401454Z",
     "shell.execute_reply": "2022-12-17T16:58:19.400422Z",
     "shell.execute_reply.started": "2022-12-17T16:58:19.396295Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 10)\n",
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "_ = [print(weight.shape) for weight in model.layers[0].weights]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "[LSTM] (Long Short-Term Memory) is a recurrent architecture published in 1997 that does not suffer from the gradient vanishing/exploding problem. This means, the network is able to capture long-term memory. The image below illustrates the architecture of a LSTM *memory cell*.\n",
    "\n",
    "<img src='image/lstm_cell.png' style='height:320px; margin:0px auto;'>\n",
    "\n",
    "The ability to manipulate information of LSTM is regulated via three gates (green node): forget $\\mathbf{f}_t$, input $\\mathbf{i}_t$ and output $\\mathbf{o}_t$. Recall that a gate is a Sigmoid function ($\\phi_1$ in this picture) followed by a multiplication. These gates give LSTM the ability to learn when to remember and when to forget, based on previous *hidden state* $\\mathbf{h}_{t-1}$ and current input data $\\mathbf{x}_t$. They all share the same formula, but with different parameters:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{f}_t &= \\phi_1(\\mathbf{W}_{fh}\\mathbf{h}_{t-1}+\\mathbf{W}_{fx}\\mathbf{x}_t+\\mathbf{b}_f) \\\\\n",
    "\\mathbf{i}_t &= \\phi_1(\\mathbf{W}_{ih}\\mathbf{h}_{t-1}+\\mathbf{W}_{ix}\\mathbf{x}_t+\\mathbf{b}_i) \\\\\n",
    "\\mathbf{o}_t &= \\phi_1(\\mathbf{W}_{oh}\\mathbf{h}_{t-1}+\\mathbf{W}_{ox}\\mathbf{x}_t+\\mathbf{b}_o) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The key component contributing to the long-term-memory ability of LSTM is the *internal state* $\\mathbf{c}_t$. This variable works like a conveyor belt running straight through the entire chain, gathering *additional* information $\\tilde{\\mathbf{c}}_t$ at each memory cell it goes through. Because new information is really mathematically added, we are assured the gradients can pass many times without vanishing or exploding. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\tilde{\\mathbf{c}}_t &= \\phi_2(\\mathbf{W}_{ch}\\mathbf{h}_{t-1}+\\mathbf{W}_{cx}\\mathbf{x}_t+\\mathbf{b}_c) \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t\\odot\\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot\\tilde{\\mathbf{c}}_t \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Finally, the current *hidden state* $\\mathbf{h}_t$ is computed by $\\tanh$-activating $\\mathbf{c}_t$ and scaling it down by the output gate $\\mathbf{o}_t$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t\\odot\\phi_2(\\mathbf{c}_t) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "[LSTM]: https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "<img src='image/lstm_steps.png' style='height:450px; margin:0px auto;'>\n",
    "\n",
    "Because it is quite hard to track what is going on by looking at the full architecture of LSTM, we will break it down into four steps:\n",
    "- Step 1, construct the *forget gate* that decides how much old data should be kept.\n",
    "- Step 2, construct the *input gate* that decides what new data should be stored.\n",
    "- Step 3, compute the *internal state* by combining two processes, *forgetting* and *receiving* information.\n",
    "- Step 4, construct the *output gate* and compute the output value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing\n",
    "In LSTM, all variables except the input have the same size ($D$), it is a hyperparameter set by Data Scientists. We can see in the formulation, there are 4 concatenations of $\\mathbf{x}$ and $\\mathbf{h}$, so that the number of parameters required to train a LSTM layer will be:\n",
    "\n",
    "$$4\\times D\\times(D+V+1)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "TensorFlow has both layer-level and cell-level implementations for LSTM via the classes\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM>LSTM</a></code> and\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell>LSTMCell</a></code>,\n",
    "with the following hyperparameters:\n",
    "- <code style='font-size:13px; color:firebrick'>units</code>: the dimensionality of output space ($D$).\n",
    "- <code style='font-size:13px; color:firebrick'>recurrent_activation</code>: the activation function for gates ($\\phi_1$), defaults to *sigmoid*.\n",
    "- <code style='font-size:13px; color:firebrick'>activation</code>: the activation function for processing data ($\\phi_2$), defaults to *tanh*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. GRU\n",
    "[Gated Recurrent Units] (GRU) was published in 2014 as an alternative that retains key idea of LSTM but is faster in computation.\n",
    "\n",
    "[Gated Recurrent Units]: https://en.wikipedia.org/wiki/Gated_recurrent_unit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "<img src='image/gru_cell.png' style='height:320px; margin:0px auto;'>\n",
    "\n",
    "GRU comes with the two-gate mechanism, *reset* ($\\mathbf{r}_t$) and *update* ($\\mathbf{z}_t$), rather than three like LSTM. They are also based on previous *hidden state* $\\mathbf{h}_{t-1}$ and current input data $\\mathbf{x}_t$ like LSTM's gates.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{r}_t &= \\phi_1(\\mathbf{W}_{rh}\\mathbf{h}_{t-1}+\\mathbf{W}_{rx}\\mathbf{x}_t+\\mathbf{b}_r) \\\\\n",
    "\\mathbf{z}_t &= \\phi_1(\\mathbf{W}_{zh}\\mathbf{h}_{t-1}+\\mathbf{W}_{zx}\\mathbf{x}_t+\\mathbf{b}_z) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Next, the new information $\\tilde{\\mathbf{h}}_t$ is computed by two processes, *receiving* new information $\\mathbf{x}_t$ and *forgetting* old information $\\mathbf{h}_{t-1}$. Then, it is added to the old state weightedly (weights are controled via the update gate) to get the current hidden state $\\mathbf{h}_t$. Unlike LSTM, GRU does not maintain the *internal state* but resembles its behaviour.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\tilde{\\mathbf{h}}_t &= \\phi_2(\\mathbf{W}_{hx}\\mathbf{x}_t+\\mathbf{W}_{hh}(\\mathbf{r}_t\\odot\\mathbf{h}_{t-1})+\\mathbf{b}_h) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{z}_t\\odot\\mathbf{h}_{t-1}+(1-\\mathbf{z}_t)\\odot\\tilde{\\mathbf{h}}_{t-1} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "The number f parameters of a GRU layer is $3\\times D\\times(D+V+1)$, as it has 3 concatenations of $\\mathbf{x}$ and $\\mathbf{h}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "TensorFlow implements GRU via\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU>GRU</a></code> and\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell>GRUCell</a></code>\n",
    "with the same hyperparameters as LSTM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Bi-directional\n",
    "All the recurrent layers have been introduced so far are uni-directional, i.e. going from the left to the right. In other words, our networks only model leftward context. This design works fine for time series data, but for text data, rightward context also matters. This problem is solved by generalizing recurrent layers to [bi-directional]. The architecture is described in the following image:\n",
    "\n",
    "<img src='image/rnn_bidirectional.png' style='height:245px; margin:20px auto;'>\n",
    "\n",
    "We can observe that a bi-directional layer is composed of a foward sub-layer and a backward sub-layer. They can be of the same type or not, and their cells are denoted $A$ and $A'$ respectively. Their corresponding outputs $\\overrightarrow{\\mathbf{h}}_t$ and $\\overleftarrow{\\mathbf{h}}_t$ are then merged into $\\mathbf{h}_t$ using various strategies, where the most common one is concatenating. Bi-directional layer is implemented in TensorFlow as a wrapper around recurrent layers via the class\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional>Bidirectional</a></code>.\n",
    "\n",
    "[bi-directional]: https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recurrent architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Seq2Seq\n",
    "[Seq2Seq] (Sequence-to-Sequence) is a Deep Learning architecture published in 2014 mainly for [neural machine translation]. It aims to transform an input sequence to a new one, both can be of arbitrary lengths. Its design is very natural to translation problems, and has resolved the fixed-length constraint in standalone recurrent layers. The Seq2Seq uses a encoder-decoder architecture, each process uses recurrent layers such as LSTM and GRU. With $(\\mathbf{x}_1,\\dots,\\mathbf{x}_M)$ a source sentence and $(\\mathbf{y}_1,\\dots,\\mathbf{y}_N)$ the target sentence, the architecture of Seq2Seq is described in the following image:\n",
    "\n",
    "The *encoder* of Seq2Seq is simply a recurrent layer where the hidden state at a time step is expressed by $(\\mathbf{x}_m,\\mathbf{e}_{m-1})\\mapsto\\mathbf{e}_m$. The last hidden state $\\mathbf{e}_M$ will be used as the output, known as the *context vector* $\\mathbf{c}$ (also called *sentence embedding* or *thought vector*).\n",
    "\n",
    "The *decoder* is a bit different, in which the hidden state has the signature $(\\mathbf{c},\\mathbf{y}_{n-1},\\mathbf{d}_{n-1})\\mapsto\\mathbf{d}_n$\n",
    "\n",
    "[Seq2Seq]: https://en.wikipedia.org/wiki/Seq2seq\n",
    "[neural machine translation]: https://en.wikipedia.org/wiki/Neural_machine_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Attention\n",
    "[Attention] implement\n",
    "<code style='font-size:13px'><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention>Attention</a></code>\n",
    "\n",
    "[Attention]: https://en.wikipedia.org/wiki/Attention_(machine_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Transformer\n",
    "[Transformer]\n",
    "\n",
    "[Transformer]: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- *bioinf.jku.at - [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)*\n",
    "- *arxiv.org - [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](https://arxiv.org/pdf/1409.1259.pdf)*\n",
    "- *arxiv.org - [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215v3.pdf)*\n",
    "- *arxiv.org - [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)*\n",
    "- *arxiv.org - [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)*\n",
    "- *arxiv.org - [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)*\n",
    "- *amitness.com - [Recurrent Keras layer](https://amitness.com/2020/04/recurrent-layers-keras/)*\n",
    "- *colah.github.io - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*\n",
    "- *d2l.ai - [Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)*\n",
    "- *d2l.ai - [Long Short-Term Memory](https://d2l.ai/chapter_recurrent-modern/lstm.html)*\n",
    "- *d2l.ai - [Gated Recurrent Units](https://d2l.ai/chapter_recurrent-modern/gru.html)*\n",
    "- *distill.pub - [Memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)*\n",
    "- *distill.pub - [Augumented RNNs](https://distill.pub/2016/augmented-rnns/)*\n",
    "- *lilianweng.github.io - [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)*\n",
    "- *blog.floydhub.com - [Attention mechanism](https://blog.floydhub.com/attention-mechanism/amp/)*\n",
    "---\n",
    "- https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "- https://www.kaggle.com/code/kredy10/simple-lstm-for-text-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
