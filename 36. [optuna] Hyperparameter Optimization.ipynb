{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9e8c3c-a01b-4416-a575-91b7a86329a2",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "In Machine Learning, there are commonly two problems (functions) that need to be optimized:\n",
    "\n",
    "$$\\begin{alignat}{3}\n",
    "f &:\\text{parameter} &&\\rightarrow\\text{loss} \\qquad &&(1) \\\\\n",
    "f &:\\text{hyperparameter} &&\\rightarrow\\text{score}\\qquad &&(2)\n",
    "\\end{alignat}$$\n",
    "\n",
    "The first problem is nothing but the training/fitting step; the loss function $\\mathcal{L}$ to be minimized is known and more importantly, *differentiable*. Therefore, a gradient-based method such as Gradient Descent can be used to estimate model parameters.\n",
    "\n",
    "The second one (also known as hyperparameter tuning) does not have a mathematical form and thus cannot be solved using Gradient Descent. The optimal set of hyperparameters is usually found either by Data Scientists using rule of thumb or by searching methods. However, each trial is expensive and we must find the optimal configurations with as few trials as possible.\n",
    "\n",
    "Scoring strategies in Machine Learning can be either *higher is better* (R2, AUC, F1,...) or *lower is better* (RMSE, MAE,...). To make things consistant, we assume all evaluation metrics are higher better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667ef03-9271-4d81-aa47-1d19e04c47ed",
   "metadata": {},
   "source": [
    "## 2.1. Exhausted search\n",
    "There are two popular methods of searching hyperparameters, Grid Search and Random Search. The idea of each method are already explained clearly in the image below, where the red area shows how much each hyperparameter contributes to model score.\n",
    "\n",
    "<img src='image/grid_random_search.png' style='width:450px; margin:20px auto;'>\n",
    "\n",
    "Grid Search goes to every possible combinations, and thus it will revisit a parameter value multiple times. This makes Grid Search have a low coverage and is very expensive, especially for algorithms with a huge number of hyperparameters such as XGBoost and LightGBM.\n",
    "\n",
    "Random Search is a more efficient method, it randomly takes a number combinations to train models. With the same number of iterations provided, Random Search can cover a wider range of values, and thus it is able to reach the optimal value that Grid Search cannot. Another advantage is that you can include less important hyperparameters in your search without increasing the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc9d32-07e7-42d7-ac03-3c8b5b4e5e30",
   "metadata": {},
   "source": [
    "## 2.2. Sequential search\n",
    "The searching methods are actually wasting a lot of useful information from previous trials. In order to take advantages of historical information, SMBO (**S**equential **M**odel-**B**ased **O**ptimization) comes to the rescue. The basic idea of SBMO can be simply summarized into two steps:\n",
    "- Build a probabilistic *surrogate model* of the black-box function $f:\\text{hyperparameter}\\rightarrow\\text{score}$ using results from observed expensive trials\n",
    "- According to the surrogate model, choose the most promising hyperparameters set to evaluate in the next expensive trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80929074-d337-4a84-acf1-785ce568a825",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "*Input*:\n",
    "- A domain, or search space of hyperparameters\n",
    "- $f$, the black-box function to be optimized, it maps hyperpameters (denoted $x$) to model score (denoted $y=f(x)$) and is very expensive to evaluate\n",
    "- $T$, the number of trials budget\n",
    "- $\\mathcal{S}$, a surrogate model which takes finished trials as input then returns a distribution of model score for each unobserved trial. Popular options GP (Gaussian Process) and TPE (Tree-structered Parzen Estimators) will be disussed in the next sections.\n",
    "- $\\mathcal{A}$, an acquisition function for deciding where the next trial should locate at\n",
    "\n",
    "*Step 1*: Create a set $\\mathcal{H}$ for storing historical trials $(x,y)$ with a randomly initialized trial $(x_0,y_0)$.\n",
    "\n",
    "*Step 2*: For each trial $t$ for $t=1,2,\\dots,T$:\n",
    "- Fit the surrogate model on $\\mathcal{H}$ to get $\\mathcal{S}_t$\n",
    "- Compute the acquisition function $\\mathcal{A}_t(x)$ using $\\mathcal{S}_t$ and $\\mathcal{H}$\n",
    "- Evaluate the most promising query point $x_t=\\arg\\max\\mathcal{A}_t(x)$\n",
    "- Train the expensive Machine Learning model using hyperparameters $x_t$ and compute its score $y_t=f(x_t)$\n",
    "- Append the result $(x_t,y_t)$ to $\\mathcal{H}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce4050-d33b-49c0-8fdd-56479fe6be87",
   "metadata": {},
   "source": [
    "### Acquisition functions\n",
    "Acquisition function is a very important component of SMBO, it defines the strategy to select the next set of hyperparameters to be used in training Machine Learning model. Acqusition functions control the balance between *exploitation* and *exploration*. In this section, we learn about 4 most popular options, using these notations:\n",
    "- $x^\\star,y^\\star$ are the best hyperparameters so far in $\\mathcal{H}$ and the associated model score\n",
    "- $\\varphi(\\cdot)$ and $\\psi(\\cdot)$ indicate the PDF and CDF, respectively\n",
    "- $\\mu_y$ and $\\sigma_y$ indicate the mean and standard deviation of model score according to the surrogate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d56f5-df82-4244-862f-87172ac5f450",
   "metadata": {},
   "source": [
    "***P**robability of **I**mprovement*. The idea of PI is very simple, it measures, at each unused configuration $x$, the probability that the corresponding score $y$ is higher than the current best score $y^\\star$. A small positive offset $\\epsilon$ is added to maintain the trade-off between *exploration* and *exploitation*.\n",
    "\n",
    "$$\\text{PI}=\\text{Prob }(y>y^\\star+\\epsilon)$$\n",
    "\n",
    "The intuition behind $\\epsilon$ is all about *uncertainty*. For certain areas, the distribution of model scores are clustered around the mean, so that only a small $\\epsilon$ penalizes their PI a lot. For this reason, higher values of $\\epsilon$ favor exploration; but if too high, the behaviour will be a lot like [Active Learning](<https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>). A balanced $\\epsilon$ will make SMBO explore *just enough* and spend trials effectively on exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf918829-7bd0-4a81-aa3b-7f74c67573b2",
   "metadata": {},
   "source": [
    "***E**xpected **I**mprovement*\n",
    "\n",
    "$$\\text{EI}=\\int_{-\\infty}^{\\infty} {(y-y^\\star-\\epsilon)\\,\\varphi(z)\\,\\text{d}z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bc8c2-fcf0-4b5a-9468-b183cd0308d2",
   "metadata": {},
   "source": [
    "*Upper Confidence Bound*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a940923-95b2-4fd4-bf0e-9e6f436d0a7f",
   "metadata": {},
   "source": [
    "*Thomson Sampling*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb0e7a-6e33-449d-bbbf-eb41acadc446",
   "metadata": {},
   "source": [
    "# 2. Bayesian Optimization\n",
    "A SBMO using Gaussian Process as the surrogate function is called **B**ayesian **O**ptimization (BO), since Gaussian Process use Bayes' rule to update the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bf87a-8173-4247-97dd-61ba6d63dc42",
   "metadata": {},
   "source": [
    "# 3. Parzen Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c8e5d-b4ee-4f3a-8549-4ddec2f467fe",
   "metadata": {},
   "source": [
    "# 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca5053-5940-419e-8636-ad2dd5456fac",
   "metadata": {},
   "source": [
    "# References\n",
    "- *papers.nips.cc* - [Algorithms for Hyper-parameters Optimization](https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf)\n",
    "- *openreview.net* - [Hyperband: Bandit-based configuration evaluation for hyperparameter optimization](https://openreview.net/pdf?id=ry18Ww5ee)\n",
    "- *medium.com* - [A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n",
    "- *medium.com* - [Hyper-parameter optimization algorithms: a short review](https://medium.com/criteo-engineering/hyper-parameter-optimization-algorithms-2fe447525903)\n",
    "- *krasserm.github.io* - [Bayesian Optimization](https://krasserm.github.io/2018/03/21/bayesian-optimization/)\n",
    "- *distill.pub* - [Exploring Bayesian Optimization](https://distill.pub/2020/bayesian-optimization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb00b13-5acf-4214-ac79-05bc4e6845f2",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
