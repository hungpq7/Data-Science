{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9e8c3c-a01b-4416-a575-91b7a86329a2",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "In Machine Learning, there are commonly two problems (functions) that need to be optimized:\n",
    "\n",
    "$$\\begin{alignat}{3}\n",
    "f &:\\text{parameter} &&\\rightarrow\\text{loss} \\qquad &&(1) \\\\\n",
    "f &:\\text{hyperparameter} &&\\rightarrow\\text{score}\\qquad &&(2)\n",
    "\\end{alignat}$$\n",
    "\n",
    "The first problem is nothing but the training/fitting step; the loss function $\\mathcal{L}$ is known and more importantly, *differentiable*. Therefore, a gradient-based method such as Gradient Descent can be used to estimate model parameters.\n",
    "\n",
    "However, the second one (also known as hyperparameter tuning) even does not have a mathematical form and thus cannot be solved using Gradient Descent. The optimal set of hyperparameters is usually found either by Data Scientists using rule of thumb or by searching methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667ef03-9271-4d81-aa47-1d19e04c47ed",
   "metadata": {},
   "source": [
    "## 1.1. Exhausted search\n",
    "There are two popular methods of searching hyperparameters, Grid Search and Random Search. The idea of each method are already explained clearly in the image below, where the red area shows how much each hyperparameter contributes to model score.\n",
    "\n",
    "<img src='image/grid_random_search.png' style='width:450px; margin:20px auto;'>\n",
    "\n",
    "Grid Search goes to every possible combinations, and thus it will revisit a parameter value multiple times. This makes Grid Search have a low coverage and is very expensive, especially for algorithms with a huge number of hyperparameters such as XGBoost and LightGBM.\n",
    "\n",
    "Random Search is a more efficient method, it randomly takes a number combinations to train models. With the same number of iterations provided, Random Search can cover a wider range of values, and thus it is able to reach the optimal value that Grid Search cannot. Another advantage is that you can include less important hyperparameters in your search without increasing the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc9d32-07e7-42d7-ac03-3c8b5b4e5e30",
   "metadata": {},
   "source": [
    "## 1.2. Sequential search\n",
    "The searching methods are actually wasting a lot of useful information from previous trials. In order to take advantages of historical information, SBMO (**S**equential **M**odel-**B**ased **O**ptimization) comes to the rescue. The basic idea of SBMO can be summarized into two steps:\n",
    "- Build a probabilistic model of the black-box function $f:\\text{hyperparameter}\\rightarrow\\text{score}$ using results from observed expensive trials\n",
    "- Choose the most promising hyperparameters set to evaluate in the next expensive trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb0e7a-6e33-449d-bbbf-eb41acadc446",
   "metadata": {},
   "source": [
    "# 2. Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ac601-4f73-4bfa-898a-4f751febfdf2",
   "metadata": {},
   "source": [
    "# 3. Tree-structured Parzen Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c8e5d-b4ee-4f3a-8549-4ddec2f467fe",
   "metadata": {},
   "source": [
    "# 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb00b13-5acf-4214-ac79-05bc4e6845f2",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
